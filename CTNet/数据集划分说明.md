# 数据集划分说明

## 总体划分流程

数据集划分分为三个层次：

### 第一层：原始数据文件划分（数据文件级别）

**数据文件来源：**
- 训练文件：`A01T.mat`, `A02T.mat`, ... （T = Train）
- 测试文件：`A01E.mat`, `A02E.mat`, ... （E = Test）

每个受试者有两个独立的文件，这是 BCI Competition IV 数据集的原始划分。

### 第二层：评估模式划分（评估策略级别）

根据 `evaluate_mode` 参数，有两种模式：

#### 模式1：Subject-Dependent（受试者内，默认）

**代码位置：** `utils.py` - `load_data_subject_dependent()`

```
对于受试者 i：
  - 训练集 = A{i}T.mat 文件中的数据
  - 测试集 = A{i}E.mat 文件中的数据
```

**特点：**
- 每个受试者独立训练和测试
- 训练集和测试集来自同一个受试者的不同session
- 用于评估受试者特定的分类性能

#### 模式2：LOSO（Leave One Subject Out，跨受试者）

**代码位置：** `utils.py` - `load_data_LOSO()`

```
对于受试者 i：
  - 训练集 = 其他8个受试者的所有数据（A01T+E, A02T+E, ..., 排除 Ai）
  - 测试集 = 受试者 i 的所有数据（AiT + AiE）
```

**特点：**
- 测试受试者的数据完全不在训练集中
- 用于评估跨受试者的泛化能力
- 需要训练9次（每个受试者作为测试集一次）

### 第三层：训练时划分（训练/验证划分）

**代码位置：** `CTNet_model.py` - `train()` 方法（第371-399行）

从**第一层或第二层**得到的训练数据（`train_data`），在训练时进一步划分为：

```python
# 原始训练数据（来自文件）
img, label = train_data, train_label  # 例如：288个样本

# 按比例划分训练集和验证集
dataset_size = len(img)              # 总样本数
val_size = int(validate_ratio * dataset_size)  # 验证集大小
train_size = dataset_size - val_size           # 训练集大小

# 随机打乱后划分
indices = torch.randperm(dataset_size)
train_indices = indices[:train_size]           # 前 train_size 个
val_indices = indices[train_size:]             # 后 val_size 个

# 最终划分
训练集 = img[train_indices]   # 例如：202个样本（如果 validate_ratio=0.3）
验证集 = img[val_indices]     # 例如：86个样本
测试集 = test_data            # 保持不变，例如：288个样本
```

## 完整示例

### Subject-Dependent 模式示例

假设训练受试者1，`validate_ratio = 0.3`：

```
原始数据文件：
  ├─ A01T.mat (288个训练样本)
  └─ A01E.mat (288个测试样本)

第二层划分（评估模式）：
  训练数据 = A01T.mat 的 288 个样本
  测试数据 = A01E.mat 的 288 个样本

第三层划分（训练时）：
  训练集 = 288 × 0.7 = 201 个样本（随机选择）
  验证集 = 288 × 0.3 = 87 个样本（剩余样本）
  测试集 = 288 个样本（保持不变，来自 A01E.mat）
```

### LOSO 模式示例

假设训练受试者1作为测试集：

```
原始数据文件：
  ├─ A01T.mat + A01E.mat → 测试集
  ├─ A02T.mat + A02E.mat → 训练数据
  ├─ A03T.mat + A03E.mat → 训练数据
  ├─ ...
  └─ A09T.mat + A09E.mat → 训练数据

第二层划分（评估模式）：
  训练数据 = 所有其他受试者的数据（约 2304 个样本 = 288 × 8）
  测试数据 = A01 的所有数据（576 个样本 = 288 × 2）

第三层划分（训练时）：
  训练集 = 2304 × 0.7 = 1613 个样本（随机选择）
  验证集 = 2304 × 0.3 = 691 个样本（剩余样本）
  测试集 = 576 个样本（保持不变，来自 A01T + A01E）
```

## 数据归一化

**重要：** 归一化是在**第三层划分之前**进行的：

```python
# 使用训练数据的均值和标准差归一化
target_mean = np.mean(allData)  # 只基于训练数据
target_std = np.std(allData)    # 只基于训练数据

# 归一化训练数据
self.allData = (self.allData - target_mean) / target_std

# 使用相同的均值和标准差归一化测试数据
self.testData = (self.testData - target_mean) / target_std
```

**关键点：**
- 测试集的归一化参数来自训练集，避免数据泄露
- 验证集的归一化已经在训练数据中完成（因为它们来自同一数据集）

## 代码位置总结

| 功能 | 文件 | 函数/方法 | 行号 |
|------|------|-----------|------|
| 加载原始mat文件 | `utils.py` | `load_data()` | 117-146 |
| 评估模式划分 | `utils.py` | `load_data_evaluate()`<br>`load_data_subject_dependent()`<br>`load_data_LOSO()` | 28-114 |
| 获取数据 | `CTNet_model.py` | `get_source_data()` | 337-364 |
| 训练/验证划分 | `CTNet_model.py` | `train()` | 371-399 |

## 配置参数

### validate_ratio（验证集比例）

**位置：** `CTNet_model.py` 文件末尾（约第786行）

```python
validate_ratio = 0.3  # 30% 用于验证，70% 用于训练
```

**建议值：**
- Subject-Dependent: 0.2 - 0.3
- LOSO: 0.2 - 0.3

### evaluate_mode（评估模式）

**位置：** `CTNet_model.py` 文件末尾（约第777行）

```python
EVALUATE_MODE = 'LOSO-No'  # 或 'LOSO'
```

- `'LOSO-No'` 或任何非 `'LOSO'` 的值：Subject-Dependent 模式
- `'LOSO'`：Leave One Subject Out 模式

## 数据增强

**重要说明：** 数据增强只应用于**训练集**，不应用于验证集和测试集。

```python
# 训练时（包含增强）
aug_data, aug_label = self.interaug(self.allData, self.allLabel)
img_aug = torch.cat((img, aug_data))  # 原始 + 增强数据

# 验证和测试时（不包含增强）
# 直接使用原始数据
```

## 常见问题

### Q1: 为什么验证集准确率可能比训练集高？

**A:** 这是正常的，因为：
- 训练准确率是在包含数据增强的数据上计算的（评估时用原始数据）
- 验证准确率是在纯原始数据上计算的
- 如果数据增强质量不高，模型在原始数据上可能表现更好

### Q2: 为什么测试准确率比验证准确率低？

**A:** 这是正常的，因为：
- 验证集来自训练数据（同一个受试者的不同session或相同数据源）
- 测试集是完全独立的数据（不同session或不同受试者）
- 测试集更能反映真实的泛化能力

### Q3: 验证集和测试集有什么区别？

**A:**
- **验证集（Validation Set）**：
  - 用于模型选择和超参数调优
  - 来自训练数据的一部分
  - 参与训练过程（选择最佳模型）
  
- **测试集（Test Set）**：
  - 用于最终性能评估
  - 完全独立于训练过程
  - 不参与模型选择
  - 反映真实泛化能力

### Q4: 如何修改验证集比例？

**A:** 在 `CTNet_model.py` 文件末尾修改：
```python
validate_ratio = 0.2  # 改为您想要的比例（0.0 - 1.0）
```




