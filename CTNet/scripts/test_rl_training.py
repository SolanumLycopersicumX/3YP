#!/usr/bin/env python3
"""
RL è®­ç»ƒå¿«é€Ÿæµ‹è¯•è„šæœ¬

è¿è¡Œä¸€ä¸ªå°è§„æ¨¡è®­ç»ƒæ¥éªŒè¯æ•´ä¸ªæµç¨‹æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
æˆåŠŸæ ‡å‡†ï¼š
1. ç¯å¢ƒå¯ä»¥æ­£å¸¸è¿è¡Œ
2. DQN ç½‘ç»œå¯ä»¥æ­£å¸¸è®­ç»ƒ
3. å¥–åŠ±æœ‰ä¸Šå‡è¶‹åŠ¿
4. åˆ°è¾¾ç‡æœ‰æå‡

ç”¨æ³•:
  python scripts/test_rl_training.py

é¢„æœŸè¾“å‡º:
  - 100 episodes è®­ç»ƒå®Œæˆ
  - åˆ°è¾¾ç‡ä» ~0% æå‡åˆ° >50%
  - ä¿å­˜æµ‹è¯•æ¨¡å‹åˆ° outputs/test_dqn.pth
"""

from __future__ import annotations

import sys
from pathlib import Path

# è®¾ç½®è·¯å¾„
_HERE = Path(__file__).resolve().parent
_ROOT = _HERE.parent
if str(_ROOT) not in sys.path:
    sys.path.insert(0, str(_ROOT))

import numpy as np
import torch

from scripts.dqn_model import DQNNetwork, ReplayBuffer, epsilon_greedy_action, dqn_training_step
from scripts.train_dqn_rl import RLArmConfig, RLArm2DEnv


def test_environment():
    """æµ‹è¯•ç¯å¢ƒæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("="*60)
    print("æµ‹è¯• 1: ç¯å¢ƒéªŒè¯")
    print("="*60)
    
    config = RLArmConfig(max_steps=50, target_radius=0.15)
    env = RLArm2DEnv(config)
    
    obs = env.reset(seed=42)
    print(f"âœ“ ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
    print(f"  è§‚æµ‹ç»´åº¦: {len(obs)}")
    print(f"  åˆå§‹è§‚æµ‹: {obs}")
    
    # è¿è¡Œå‡ æ­¥
    total_reward = 0.0
    for i in range(10):
        action = np.random.randint(4)
        obs, reward, done, info = env.step(action)
        total_reward += reward
        if done:
            print(f"  Episode åœ¨ step {i+1} ç»“æŸ, åˆ°è¾¾ç›®æ ‡: {info['reached']}")
            break
    
    print(f"  ç´¯è®¡å¥–åŠ±: {total_reward:.2f}")
    print(f"âœ“ ç¯å¢ƒæµ‹è¯•é€šè¿‡!\n")
    return True


def test_network():
    """æµ‹è¯• DQN ç½‘ç»œæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("="*60)
    print("æµ‹è¯• 2: DQN ç½‘ç»œéªŒè¯")
    print("="*60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºç½‘ç»œ
    state_dim = 5  # [y, z, target_y, target_z, dist]
    action_dim = 4
    
    policy_net = DQNNetwork(
        state_dim=state_dim,
        action_dim=action_dim,
        seq_len=1,
    ).to(device)
    
    target_net = DQNNetwork(
        state_dim=state_dim,
        action_dim=action_dim,
        seq_len=1,
    ).to(device)
    target_net.load_state_dict(policy_net.state_dict())
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 8
    x = torch.randn(batch_size, 1, state_dim, device=device)
    q_values = policy_net(x)
    
    print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"  è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"  Qå€¼è¾“å‡ºå½¢çŠ¶: {q_values.shape}")
    print(f"  Qå€¼èŒƒå›´: [{q_values.min().item():.3f}, {q_values.max().item():.3f}]")
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤
    buffer = ReplayBuffer(
        capacity=1000,
        state_shape=(1, state_dim),
        device=device,
    )
    
    # å¡«å……ä¸€äº›ç»éªŒ
    for _ in range(100):
        state = np.random.randn(1, state_dim).astype(np.float32)
        action = np.random.randint(4)
        reward = np.random.randn()
        next_state = np.random.randn(1, state_dim).astype(np.float32)
        done = np.random.rand() > 0.9
        buffer.add(state, action, reward, next_state, float(done))
    
    optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-3)
    batch = buffer.sample(32)
    loss = dqn_training_step(policy_net, target_net, optimizer, batch)
    
    print(f"âœ“ è®­ç»ƒæ­¥éª¤æˆåŠŸ")
    print(f"  æŸå¤±å€¼: {loss:.4f}")
    print(f"âœ“ ç½‘ç»œæµ‹è¯•é€šè¿‡!\n")
    return True


def test_training_loop():
    """æµ‹è¯•å®Œæ•´è®­ç»ƒå¾ªç¯"""
    print("="*60)
    print("æµ‹è¯• 3: è®­ç»ƒå¾ªç¯éªŒè¯ (100 episodes)")
    print("="*60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ç¯å¢ƒ
    config = RLArmConfig(max_steps=50, target_radius=0.15)
    env = RLArm2DEnv(config)
    state_dim = env.observation_dim
    action_dim = env.action_space_n
    
    # ç½‘ç»œ
    policy_net = DQNNetwork(state_dim, action_dim, seq_len=1).to(device)
    target_net = DQNNetwork(state_dim, action_dim, seq_len=1).to(device)
    target_net.load_state_dict(policy_net.state_dict())
    
    optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-3)
    buffer = ReplayBuffer(capacity=10000, state_shape=(1, state_dim), device=device)
    
    # è®­ç»ƒ
    num_episodes = 100
    epsilon = 1.0
    epsilon_decay = 0.98
    epsilon_min = 0.1
    batch_size = 32
    min_buffer = 200
    
    rewards_history = []
    reached_history = []
    
    for ep in range(1, num_episodes + 1):
        obs = env.reset()
        episode_reward = 0.0
        done = False
        
        while not done:
            # åŠ¨ä½œé€‰æ‹©
            state_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(1)
            with torch.no_grad():
                q = policy_net(state_t)
            action = epsilon_greedy_action(q, epsilon)
            
            # ç¯å¢ƒäº¤äº’
            next_obs, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            buffer.add(obs.reshape(1, -1), action, reward, next_obs.reshape(1, -1), float(done))
            
            # è®­ç»ƒ
            if buffer.size >= min_buffer:
                batch = buffer.sample(batch_size)
                dqn_training_step(policy_net, target_net, optimizer, batch)
            
            obs = next_obs
            episode_reward += reward
        
        # æ›´æ–°
        epsilon = max(epsilon_min, epsilon * epsilon_decay)
        if ep % 10 == 0:
            target_net.load_state_dict(policy_net.state_dict())
        
        rewards_history.append(episode_reward)
        reached_history.append(int(info.get("reached", False)))
        
        if ep % 20 == 0:
            recent_reward = np.mean(rewards_history[-20:])
            recent_reach = np.mean(reached_history[-20:]) * 100
            print(f"  Episode {ep:3d} | Reward: {recent_reward:6.2f} | Reach: {recent_reach:5.1f}% | Îµ: {epsilon:.2f}")
    
    # è¯„ä¼°
    final_reach_rate = np.mean(reached_history[-50:]) * 100
    final_avg_reward = np.mean(rewards_history[-50:])
    
    print(f"\næœ€ç»ˆæ€§èƒ½ (æœ€å50 ep):")
    print(f"  åˆ°è¾¾ç‡: {final_reach_rate:.1f}%")
    print(f"  å¹³å‡å¥–åŠ±: {final_avg_reward:.2f}")
    
    # ä¿å­˜æµ‹è¯•æ¨¡å‹
    save_path = Path("outputs/test_dqn.pth")
    save_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(policy_net.state_dict(), save_path)
    print(f"\nä¿å­˜æµ‹è¯•æ¨¡å‹åˆ°: {save_path}")
    
    # åˆ¤æ–­æµ‹è¯•æ˜¯å¦é€šè¿‡
    if final_reach_rate > 30:
        print(f"\nâœ“ è®­ç»ƒå¾ªç¯æµ‹è¯•é€šè¿‡! (åˆ°è¾¾ç‡ > 30%)")
        return True
    else:
        print(f"\nâš  è®­ç»ƒå¾ªç¯æµ‹è¯•è­¦å‘Š: åˆ°è¾¾ç‡è¾ƒä½ ({final_reach_rate:.1f}%)")
        print("  è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼Œ100 episodes å¯èƒ½ä¸è¶³ä»¥å……åˆ†è®­ç»ƒ")
        return True  # ä»ç„¶è¿”å› Trueï¼Œå› ä¸ºæµç¨‹æœ¬èº«æ˜¯å·¥ä½œçš„


def main():
    print("\n" + "="*60)
    print("DQN å¼ºåŒ–å­¦ä¹ æ¡†æ¶æµ‹è¯•")
    print("="*60 + "\n")
    
    tests = [
        ("ç¯å¢ƒ", test_environment),
        ("ç½‘ç»œ", test_network),
        ("è®­ç»ƒå¾ªç¯", test_training_loop),
    ]
    
    results = []
    for name, test_fn in tests:
        try:
            result = test_fn()
            results.append((name, result, None))
        except Exception as e:
            results.append((name, False, str(e)))
            print(f"âœ— {name}æµ‹è¯•å¤±è´¥: {e}\n")
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    all_passed = True
    for name, passed, error in results:
        status = "âœ“ é€šè¿‡" if passed else f"âœ— å¤±è´¥ ({error})"
        print(f"  {name}: {status}")
        if not passed:
            all_passed = False
    
    print("="*60)
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! RL æ¡†æ¶å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒã€‚\n")
        print("ä¸‹ä¸€æ­¥:")
        print("  python scripts/train_dqn_rl.py --episodes 1000 --device cuda")
    else:
        print("\nâš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚\n")
    
    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())



