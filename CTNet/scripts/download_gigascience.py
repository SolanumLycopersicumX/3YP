#!/usr/bin/env python3
"""
GigaScience æ•°æ®é›†éƒ¨åˆ†ä¸‹è½½è„šæœ¬

å°è¯•ä» GigaDB åªä¸‹è½½å‰ N ä¸ªè¢«è¯•çš„æ•°æ®ï¼Œè€Œä¸æ˜¯æ•´ä¸ª 226GB å‹ç¼©åŒ…

æ•°æ®é›†: http://gigadb.org/dataset/100788
è®ºæ–‡: Jeong et al., "Multimodal signal dataset for 11 intuitive movement tasks", GigaScience, 2020

åˆ›å»ºæ—¶é—´: 2026-02-10

============================================================================
âš ï¸ é‡è¦å‘ç°:
============================================================================
ç»è¿‡æµ‹è¯•ï¼ŒGigaDB çš„ EEG_ConvertedData.tar.gz (226GB) æ˜¯ä¸€ä¸ªå®Œæ•´æ‰“åŒ…çš„æ–‡ä»¶ï¼Œ
æ— æ³•ç›´æ¥æŒ‰è¢«è¯•å•ç‹¬ä¸‹è½½ã€‚

æ›¿ä»£æ–¹æ¡ˆï¼š
1. ä½¿ç”¨ IV-2a å’Œ IV-2b æ•°æ®é›†ï¼ˆæ¨èï¼Œå·²æœ‰è‰¯å¥½ç»“æœï¼‰
2. ä¸‹è½½å‹ç¼©åŒ…åä½¿ç”¨ tar é€‰æ‹©æ€§è§£å‹
3. ä½¿ç”¨å…¶ä»–è¾ƒå°çš„å…¬å¼€æ•°æ®é›†ï¼ˆå¦‚ OpenBMIï¼‰
============================================================================
"""

from __future__ import annotations

import argparse
import os
import re
import subprocess
import urllib.request
from ftplib import FTP
from pathlib import Path
from typing import List, Optional, Tuple
from html.parser import HTMLParser
import ssl


# ============================================================================
# é…ç½®
# ============================================================================

GIGADB_DATASET_ID = "100788"

# å¯èƒ½çš„ FTP/HTTP åœ°å€ï¼ˆæ ¹æ® GigaDB æ–‡æ¡£ï¼‰
POTENTIAL_HOSTS = [
    ("ftp.cngb.org", f"/pub/gigadb/pub/10.5524/{GIGADB_DATASET_ID}/"),
    ("parrot.genomics.cn", f"/gigadb/pub/10.5524/100001_101000/{GIGADB_DATASET_ID}/"),
]

POTENTIAL_HTTP_URLS = [
    f"https://ftp.cngb.org/pub/gigadb/pub/10.5524/{GIGADB_DATASET_ID}/",
    f"http://parrot.genomics.cn/gigadb/pub/10.5524/100001_101000/{GIGADB_DATASET_ID}/",
    f"https://gigadb.org/dataset/100788",
]


# ============================================================================
# HTML ç›®å½•è§£æå™¨
# ============================================================================

class DirectoryParser(HTMLParser):
    """è§£æ HTTP ç›®å½•åˆ—è¡¨"""
    
    def __init__(self):
        super().__init__()
        self.links = []
    
    def handle_starttag(self, tag, attrs):
        if tag == 'a':
            for attr, value in attrs:
                if attr == 'href' and value and not value.startswith('?'):
                    self.links.append(value)


def list_http_directory(url: str) -> List[str]:
    """åˆ—å‡º HTTP ç›®å½•ä¸­çš„æ–‡ä»¶"""
    try:
        # åˆ›å»ºä¸éªŒè¯ SSL çš„ä¸Šä¸‹æ–‡ï¼ˆæŸäº›æœåŠ¡å™¨è¯ä¹¦å¯èƒ½æœ‰é—®é¢˜ï¼‰
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
        with urllib.request.urlopen(req, context=context, timeout=30) as response:
            html = response.read().decode('utf-8')
        
        parser = DirectoryParser()
        parser.feed(html)
        
        # è¿‡æ»¤æ‰çˆ¶ç›®å½•é“¾æ¥
        files = [f for f in parser.links if not f.startswith('/') and f != '../']
        return files
    
    except Exception as e:
        print(f"HTTP ç›®å½•è®¿é—®å¤±è´¥: {e}")
        return []


def list_ftp_directory(host: str, path: str) -> List[Tuple[str, int]]:
    """åˆ—å‡º FTP ç›®å½•ä¸­çš„æ–‡ä»¶å’Œå¤§å°"""
    try:
        ftp = FTP(host, timeout=30)
        ftp.login()  # åŒ¿åç™»å½•
        ftp.cwd(path)
        
        files = []
        
        def callback(line):
            parts = line.split()
            if len(parts) >= 9:
                size = int(parts[4]) if parts[4].isdigit() else 0
                name = parts[-1]
                files.append((name, size))
        
        ftp.retrlines('LIST', callback)
        ftp.quit()
        
        return files
    
    except Exception as e:
        print(f"FTP ç›®å½•è®¿é—®å¤±è´¥: {e}")
        return []


# ============================================================================
# æ¢ç´¢æ•°æ®é›†ç»“æ„
# ============================================================================

def explore_dataset_structure():
    """æ¢ç´¢ GigaScience æ•°æ®é›†çš„ç›®å½•ç»“æ„"""
    
    print("="*60)
    print("GigaScience æ•°æ®é›†ç»“æ„æ¢ç´¢")
    print("="*60)
    print(f"æ•°æ®é›† ID: {GIGADB_DATASET_ID}")
    print(f"æ•°æ®é›†é¡µé¢: https://gigadb.org/dataset/{GIGADB_DATASET_ID}")
    print()
    
    all_files = []
    
    # å°è¯• HTTP è®¿é—®
    print("[1] å°è¯• HTTP è®¿é—®...")
    for url in POTENTIAL_HTTP_URLS:
        if "gigadb.org/dataset" in url:
            print(f"  è·³è¿‡æ•°æ®é›†é¡µé¢ (éœ€è¦ JavaScript): {url}")
            continue
        
        print(f"  å°è¯•: {url}")
        http_files = list_http_directory(url)
        
        if http_files:
            print(f"  âœ“ æ‰¾åˆ° {len(http_files)} ä¸ªæ–‡ä»¶/ç›®å½•:")
            for f in http_files[:10]:
                print(f"    - {f}")
            if len(http_files) > 10:
                print(f"    ... è¿˜æœ‰ {len(http_files) - 10} ä¸ª")
            all_files.extend(http_files)
            break
    
    if not all_files:
        print("  âœ— æ‰€æœ‰ HTTP URL è®¿é—®å¤±è´¥")
    
    # å°è¯• FTP è®¿é—®
    print("\n[2] å°è¯• FTP è®¿é—®...")
    for host, path in POTENTIAL_HOSTS:
        print(f"  å°è¯•: ftp://{host}{path}")
        ftp_files = list_ftp_directory(host, path)
        
        if ftp_files:
            print(f"  âœ“ æ‰¾åˆ° {len(ftp_files)} ä¸ªæ–‡ä»¶/ç›®å½•:")
            for name, size in ftp_files[:10]:
                size_str = format_size(size)
                print(f"    - {name} ({size_str})")
            if len(ftp_files) > 10:
                print(f"    ... è¿˜æœ‰ {len(ftp_files) - 10} ä¸ª")
            all_files.extend([f[0] for f in ftp_files])
            break
    
    if not all_files:
        print("  âœ— æ‰€æœ‰ FTP åœ°å€è®¿é—®å¤±è´¥")
        print("\n  âš ï¸ å¯èƒ½åŸå› :")
        print("    - ç½‘ç»œé™åˆ¶/é˜²ç«å¢™")
        print("    - FTP æœåŠ¡å™¨åœ°å€å·²æ›´æ”¹")
        print("    - éœ€è¦ä»£ç†/VPN")
    
    # åˆ†ææ˜¯å¦å¯ä»¥æŒ‰è¢«è¯•ä¸‹è½½
    print("\n[3] åˆ†æå·²çŸ¥ä¿¡æ¯...")
    
    if all_files:
        # æŸ¥æ‰¾è¢«è¯•ç›¸å…³çš„æ–‡ä»¶æ¨¡å¼
        subject_patterns = [
            r'[Ss]ub\d+',
            r'[Ss]\d+',
            r'subject\d+',
            r'[Ss]ubject_\d+',
        ]
        
        subject_files = []
        for f in all_files:
            for pattern in subject_patterns:
                if re.search(pattern, f, re.IGNORECASE):
                    subject_files.append(f)
                    break
        
        if subject_files:
            print(f"  âœ“ æ‰¾åˆ° {len(subject_files)} ä¸ªå¯èƒ½çš„è¢«è¯•æ–‡ä»¶:")
            for f in subject_files[:10]:
                print(f"    - {f}")
            print("\n  âœ“ å¯ä»¥æŒ‰è¢«è¯•å•ç‹¬ä¸‹è½½!")
            return True, subject_files
        else:
            # æ£€æŸ¥æ˜¯å¦åªæœ‰å¤§çš„å‹ç¼©åŒ…
            tar_files = [f for f in all_files if '.tar' in f.lower() or '.gz' in f.lower()]
            if tar_files:
                print(f"  âš  åªæ‰¾åˆ°å‹ç¼©åŒ…æ–‡ä»¶:")
                for f in tar_files:
                    print(f"    - {f}")
                print("\n  âœ— æ•°æ®æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œæ— æ³•æŒ‰è¢«è¯•ä¸‹è½½")
                return False, tar_files
    
    # æ ¹æ®å·²çŸ¥ä¿¡æ¯æä¾›ç»“è®º
    print("\n  ğŸ“‹ æ ¹æ® GigaDB é¡µé¢ä¿¡æ¯:")
    print("    - EEG_ConvertedData.tar.gz (226.32 GB) - MATLAB æ ¼å¼ EEG æ•°æ®")
    print("    - RawData.tar.gz (211.89 GB) - åŸå§‹ EEG/EMG/EOG æ•°æ®")
    print("\n  âœ— æ•°æ®è¢«æ‰“åŒ…åœ¨å¤§å‹å‹ç¼©æ–‡ä»¶ä¸­ï¼Œæ— æ³•ç›´æ¥æŒ‰è¢«è¯•ä¸‹è½½")
    
    return False, ["EEG_ConvertedData.tar.gz", "RawData.tar.gz"]


def format_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.2f} PB"


# ============================================================================
# ä¸‹è½½åŠŸèƒ½
# ============================================================================

def download_file(url: str, output_path: Path, show_progress: bool = True):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    context = ssl.create_default_context()
    context.check_hostname = False
    context.verify_mode = ssl.CERT_NONE
    
    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    
    with urllib.request.urlopen(req, context=context) as response:
        total_size = int(response.headers.get('content-length', 0))
        
        with open(output_path, 'wb') as f:
            downloaded = 0
            block_size = 8192
            
            while True:
                buffer = response.read(block_size)
                if not buffer:
                    break
                
                f.write(buffer)
                downloaded += len(buffer)
                
                if show_progress and total_size > 0:
                    percent = downloaded / total_size * 100
                    print(f"\r  ä¸‹è½½è¿›åº¦: {percent:.1f}% ({format_size(downloaded)}/{format_size(total_size)})", end='')
            
            if show_progress:
                print()


def download_subjects(
    subject_files: List[str],
    subjects_to_download: List[int],
    output_dir: Path,
    base_url: str,
):
    """ä¸‹è½½æŒ‡å®šè¢«è¯•çš„æ•°æ®"""
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nä¸‹è½½ {len(subjects_to_download)} ä¸ªè¢«è¯•çš„æ•°æ®...")
    
    for subject in subjects_to_download:
        # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        patterns = [
            f'sub{subject:02d}',
            f'Sub{subject:02d}',
            f's{subject:02d}',
            f'S{subject:02d}',
            f'subject{subject:02d}',
            f'Subject{subject:02d}',
        ]
        
        matching_files = []
        for f in subject_files:
            for p in patterns:
                if p in f:
                    matching_files.append(f)
                    break
        
        if not matching_files:
            print(f"  Subject {subject}: æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶")
            continue
        
        for file in matching_files:
            url = base_url + file
            output_path = output_dir / file
            
            print(f"  Subject {subject}: ä¸‹è½½ {file}...")
            
            try:
                download_file(url, output_path)
                print(f"    âœ“ ä¿å­˜åˆ° {output_path}")
            except Exception as e:
                print(f"    âœ— ä¸‹è½½å¤±è´¥: {e}")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def parse_args():
    p = argparse.ArgumentParser(description="GigaScience æ•°æ®é›†éƒ¨åˆ†ä¸‹è½½")
    
    p.add_argument("--explore", action="store_true",
                   help="åªæ¢ç´¢ç›®å½•ç»“æ„ï¼Œä¸ä¸‹è½½")
    p.add_argument("--subjects", type=int, nargs="+", default=[1, 2, 3],
                   help="è¦ä¸‹è½½çš„è¢«è¯•ç¼–å·")
    p.add_argument("--output-dir", type=Path, default=Path("./gigascience_raw/"),
                   help="è¾“å‡ºç›®å½•")
    
    return p.parse_args()


def check_partial_tar_support():
    """æ£€æŸ¥æ˜¯å¦å¯ä»¥éƒ¨åˆ†è§£å‹ tar.gz æ–‡ä»¶"""
    print("\n" + "="*60)
    print("æ£€æŸ¥éƒ¨åˆ†è§£å‹æ”¯æŒ")
    print("="*60)
    
    # æµ‹è¯• curl æ˜¯å¦æ”¯æŒ range è¯·æ±‚
    try:
        result = subprocess.run(
            ["curl", "--version"],
            capture_output=True, text=True, timeout=10
        )
        print("  âœ“ curl å¯ç”¨")
    except Exception:
        print("  âœ— curl ä¸å¯ç”¨")
    
    # æµ‹è¯• tar æ˜¯å¦æ”¯æŒ wildcards
    try:
        result = subprocess.run(
            ["tar", "--help"],
            capture_output=True, text=True, timeout=10
        )
        if "--wildcards" in result.stdout:
            print("  âœ“ tar æ”¯æŒ --wildcards é€‰é¡¹")
        else:
            print("  âš  tar å¯èƒ½ä¸æ”¯æŒ --wildcards")
    except Exception:
        print("  âœ— tar ä¸å¯ç”¨")


def print_alternatives():
    """æ‰“å°æ›¿ä»£æ–¹æ¡ˆ"""
    print("\n" + "="*60)
    print("ğŸ“‹ æ›¿ä»£æ–¹æ¡ˆ")
    print("="*60)
    print("""
ç”±äº GigaScience æ•°æ®æ‰“åŒ…åœ¨ä¸€ä¸ª 226GB çš„å‹ç¼©åŒ…ä¸­ï¼Œæ— æ³•ç›´æ¥æŒ‰è¢«è¯•å•ç‹¬ä¸‹è½½ã€‚

==============================
æ–¹æ¡ˆ 1: ä½¿ç”¨ç°æœ‰æ•°æ®é›†ï¼ˆæ¨èï¼‰
==============================
ä½ å·²ç»æœ‰ IV-2a (4ç±», 22é€šé“) å’Œ IV-2b (2ç±», 3é€šé“) çš„è‰¯å¥½ç»“æœï¼š
  - IV-2a: åˆ†ç±»å‡†ç¡®ç‡ ~77%, æ§åˆ¶åˆ°è¾¾ç‡ ~99%
  - IV-2b: åˆ†ç±»å‡†ç¡®ç‡ ~73%, æ§åˆ¶åˆ°è¾¾ç‡ ~93%

è¿™è¶³ä»¥è¯æ˜ä½ çš„ RL æ§åˆ¶æ¡†æ¶æœ‰æ•ˆï¼

==============================
æ–¹æ¡ˆ 2: éƒ¨åˆ†è§£å‹ï¼ˆå¦‚æœå·²ä¸‹è½½ï¼‰
==============================
å¦‚æœä½ èƒ½è·å–å‹ç¼©åŒ…ï¼Œå¯ä»¥åªè§£å‹å‰3ä¸ªè¢«è¯•ï¼š

# 1. å…ˆæŸ¥çœ‹å‹ç¼©åŒ…å†…å®¹ç»“æ„
tar -tzf EEG_ConvertedData.tar.gz | head -100

# 2. åªè§£å‹åŒ¹é…çš„æ–‡ä»¶
tar -xzf EEG_ConvertedData.tar.gz --wildcards '*sub01*' '*sub02*' '*sub03*'

# æˆ–è€…è§£å‹åˆ°æŒ‡å®šç›®å½•
tar -xzf EEG_ConvertedData.tar.gz -C ./gigascience_raw/ --wildcards '*sub01*'

==============================
æ–¹æ¡ˆ 3: ä½¿ç”¨å…¶ä»–å…¬å¼€æ•°æ®é›†
==============================
- OpenBMI Dataset (~10 GB):
  https://doi.org/10.5524/100542

- PhysioNet Motor Imagery (~5 GB):
  https://physionet.org/content/eegmmidb/1.0.0/

- BNCI Horizon 2020 (~2 GB):
  http://bnci-horizon-2020.eu/database/data-sets

==============================
æ–¹æ¡ˆ 4: åœ¨ Methodology ä¸­è¯´æ˜
==============================
åœ¨è®ºæ–‡ä¸­è¯´æ˜ï¼š
"Due to the prohibitive size of the GigaScience dataset (226 GB),
we validated our approach on the BCI Competition IV datasets (2a and 2b),
which provide complementary evaluation scenarios..."

è¿™æ˜¯å­¦æœ¯ä¸Šå®Œå…¨å¯æ¥å—çš„åšæ³•ï¼
""")


def main():
    args = parse_args()
    
    # æ¢ç´¢ç›®å½•ç»“æ„
    can_download_partial, files = explore_dataset_structure()
    
    # æ£€æŸ¥éƒ¨åˆ†è§£å‹æ”¯æŒ
    check_partial_tar_support()
    
    if args.explore:
        print_alternatives()
        return
    
    if can_download_partial:
        print("\n" + "="*60)
        print("å¼€å§‹ä¸‹è½½...")
        print("="*60)
        
        download_subjects(
            files,
            args.subjects,
            args.output_dir,
            POTENTIAL_HTTP_URLS[0],
        )
        
        print("\nä¸‹è½½å®Œæˆ!")
    else:
        print_alternatives()


if __name__ == "__main__":
    main()

