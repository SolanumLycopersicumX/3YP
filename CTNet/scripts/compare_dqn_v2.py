#!/usr/bin/env python3
"""
DQN ç½‘ç»œæ¶æ„å¯¹æ¯”å®éªŒ V2 - æ”¹è¿›ç‰ˆ

ä¿®å¤ V1 ä¸­å‘ç°çš„è®­ç»ƒå´©æºƒé—®é¢˜ï¼š
1. æ›´æ…¢çš„ Îµ è¡°å‡
2. æ·»åŠ  Gradient Clipping
3. æ›´é¢‘ç¹çš„ Target Network æ›´æ–°
4. æ·»åŠ  Double DQN
5. æ·»åŠ  Prioritized Experience Replay (ç®€åŒ–ç‰ˆ)

ç”¨æ³•:
  python scripts/compare_dqn_v2.py --episodes 2000 --device cuda
"""

from __future__ import annotations

import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import sys
_HERE = Path(__file__).resolve().parent
_ROOT = _HERE.parent
if str(_ROOT) not in sys.path:
    sys.path.insert(0, str(_ROOT))

from scripts.dqn_model import DQNNetwork, ReplayBuffer
from scripts.dqn_transformer import TransformerDQN, LightTransformerDQN
from scripts.train_dqn_rl import RLArmConfig, RLArm2DEnv


# ============================================================================
# æ”¹è¿›çš„è®­ç»ƒç»„ä»¶
# ============================================================================

def epsilon_greedy_action_v2(q_values: torch.Tensor, epsilon: float) -> int:
    """Îµ-greedy åŠ¨ä½œé€‰æ‹©"""
    if np.random.rand() < epsilon:
        return np.random.randint(q_values.shape[-1])
    return int(torch.argmax(q_values).item())


def double_dqn_training_step(
    policy_net: nn.Module,
    target_net: nn.Module,
    optimizer: torch.optim.Optimizer,
    batch: Tuple,
    *,
    gamma: float = 0.99,
    grad_clip: float = 1.0,
) -> float:
    """
    Double DQN è®­ç»ƒæ­¥éª¤
    
    æ”¹è¿›ï¼šä½¿ç”¨ policy_net é€‰æ‹©åŠ¨ä½œï¼Œtarget_net è¯„ä¼° Q å€¼
    è¿™å‡å°‘äº† Q å€¼è¿‡é«˜ä¼°è®¡çš„é—®é¢˜
    """
    states, actions, rewards, next_states, dones = batch
    
    # å½“å‰ Q å€¼
    q_values = policy_net(states)
    q_values = q_values.gather(1, actions.view(-1, 1)).squeeze(1)
    
    with torch.no_grad():
        # Double DQN: ç”¨ policy_net é€‰åŠ¨ä½œ
        next_actions = policy_net(next_states).argmax(dim=1, keepdim=True)
        # ç”¨ target_net è¯„ä¼° Q å€¼
        next_q = target_net(next_states).gather(1, next_actions).squeeze(1)
        target = rewards + (1 - dones) * gamma * next_q
    
    # Huber Loss
    loss = F.smooth_l1_loss(q_values, target, reduction="mean")
    
    optimizer.zero_grad()
    loss.backward()
    
    # Gradient Clipping
    nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=grad_clip)
    
    optimizer.step()
    
    return loss.item()


def linear_schedule(start: float, end: float, current: int, total: int) -> float:
    """çº¿æ€§è°ƒåº¦å™¨"""
    progress = min(1.0, current / total)
    return start + (end - start) * progress


def exponential_schedule(start: float, end: float, decay: float, current: int) -> float:
    """æŒ‡æ•°è¡°å‡è°ƒåº¦å™¨"""
    return max(end, start * (decay ** current))


# ============================================================================
# æ”¹è¿›çš„è®­ç»ƒå‡½æ•°
# ============================================================================

def train_network_v2(
    network_name: str,
    network: nn.Module,
    env: RLArm2DEnv,
    *,
    num_episodes: int = 2000,
    batch_size: int = 64,
    gamma: float = 0.99,
    epsilon_start: float = 1.0,
    epsilon_end: float = 0.05,
    epsilon_decay_episodes: int = 1500,  # Îµ è¡°å‡åˆ°æœ€å°å€¼æ‰€éœ€ episodes
    target_update_freq: int = 5,         # æ›´é¢‘ç¹æ›´æ–° target
    soft_update_tau: float = 0.005,      # è½¯æ›´æ–°ç³»æ•°
    use_soft_update: bool = True,
    min_buffer_size: int = 1000,
    buffer_size: int = 100000,
    lr: float = 3e-4,                    # è¾ƒå°å­¦ä¹ ç‡
    grad_clip: float = 1.0,
    device: torch.device = torch.device("cpu"),
    log_interval: int = 100,
) -> Dict[str, Any]:
    """
    æ”¹è¿›çš„è®­ç»ƒå‡½æ•° V2
    """
    print(f"\n{'='*60}")
    print(f"è®­ç»ƒ: {network_name}")
    print(f"å‚æ•°é‡: {sum(p.numel() for p in network.parameters() if p.requires_grad):,}")
    print(f"æ”¹è¿›: Double DQN, Soft Update (Ï„={soft_update_tau}), Slower Îµ decay")
    print(f"{'='*60}")
    
    state_dim = env.observation_dim
    
    # åˆ›å»º target network (ä½¿ç”¨ç›¸åŒé…ç½®)
    import copy
    target_net = copy.deepcopy(network).to(device)
    target_net.load_state_dict(network.state_dict())
    target_net.eval()
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(network.parameters(), lr=lr, weight_decay=1e-5)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_episodes, eta_min=lr * 0.1
    )
    
    # Replay Buffer
    replay_buffer = ReplayBuffer(
        capacity=buffer_size,
        state_shape=(1, state_dim),
        device=device,
    )
    
    # è®­ç»ƒè®°å½•
    logs = {
        "episode": [],
        "reward": [],
        "steps": [],
        "epsilon": [],
        "loss": [],
        "reached": [],
        "lr": [],
    }
    
    start_time = time.time()
    best_reach_rate = 0.0
    best_weights = None
    
    for ep in range(1, num_episodes + 1):
        obs = env.reset()
        episode_reward = 0.0
        episode_steps = 0
        episode_losses = []
        reached = False
        
        # çº¿æ€§ Îµ è¡°å‡
        epsilon = linear_schedule(
            epsilon_start, epsilon_end, ep, epsilon_decay_episodes
        )
        
        done = False
        while not done:
            # åŠ¨ä½œé€‰æ‹©
            state_tensor = torch.tensor(obs, dtype=torch.float32, device=device)
            state_tensor = state_tensor.unsqueeze(0).unsqueeze(1)
            
            with torch.no_grad():
                q_values = network(state_tensor)
            action = epsilon_greedy_action_v2(q_values, epsilon)
            
            # ç¯å¢ƒäº¤äº’
            next_obs, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            replay_buffer.add(
                obs.reshape(1, -1), action, reward,
                next_obs.reshape(1, -1), float(done)
            )
            
            # è®­ç»ƒ
            if replay_buffer.size >= min_buffer_size:
                batch = replay_buffer.sample(batch_size)
                loss = double_dqn_training_step(
                    network, target_net, optimizer, batch,
                    gamma=gamma, grad_clip=grad_clip
                )
                episode_losses.append(loss)
            
            obs = next_obs
            episode_reward += reward
            episode_steps += 1
            
            if info.get("reached", False):
                reached = True
        
        # è½¯æ›´æ–° Target Network
        if use_soft_update:
            with torch.no_grad():
                for param, target_param in zip(network.parameters(), target_net.parameters()):
                    target_param.data.copy_(
                        soft_update_tau * param.data + (1 - soft_update_tau) * target_param.data
                    )
        elif ep % target_update_freq == 0:
            target_net.load_state_dict(network.state_dict())
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # è®°å½•
        logs["episode"].append(ep)
        logs["reward"].append(episode_reward)
        logs["steps"].append(episode_steps)
        logs["epsilon"].append(epsilon)
        logs["loss"].append(np.mean(episode_losses) if episode_losses else 0.0)
        logs["reached"].append(int(reached))
        logs["lr"].append(scheduler.get_last_lr()[0])
        
        # ä¿å­˜æœ€ä½³æƒé‡
        if ep >= 100:
            recent_reach = np.mean(logs["reached"][-100:])
            if recent_reach > best_reach_rate:
                best_reach_rate = recent_reach
                best_weights = {k: v.cpu().clone() for k, v in network.state_dict().items()}
        
        # æ‰“å°è¿›åº¦
        if ep % log_interval == 0:
            recent_rewards = logs["reward"][-log_interval:]
            recent_reached = logs["reached"][-log_interval:]
            print(
                f"  Episode {ep:4d} | "
                f"Reward: {np.mean(recent_rewards):6.2f} | "
                f"Reach: {np.mean(recent_reached)*100:5.1f}% | "
                f"Îµ: {epsilon:.3f} | "
                f"Best: {best_reach_rate*100:.1f}%"
            )
    
    elapsed = time.time() - start_time
    
    # æ¢å¤æœ€ä½³æƒé‡
    if best_weights is not None:
        network.load_state_dict({k: v.to(device) for k, v in best_weights.items()})
        print(f"  æ¢å¤æœ€ä½³æƒé‡ (reach rate: {best_reach_rate*100:.1f}%)")
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ï¼ˆä½¿ç”¨æœ€ä½³æƒé‡è¯„ä¼°ï¼‰
    eval_reached = []
    eval_rewards = []
    for _ in range(100):
        obs = env.reset()
        ep_reward = 0.0
        done = False
        while not done:
            state_tensor = torch.tensor(obs, dtype=torch.float32, device=device)
            state_tensor = state_tensor.unsqueeze(0).unsqueeze(1)
            with torch.no_grad():
                q_values = network(state_tensor)
            action = int(torch.argmax(q_values).item())  # Greedy
            obs, reward, done, info = env.step(action)
            ep_reward += reward
        eval_reached.append(int(info.get("reached", False)))
        eval_rewards.append(ep_reward)
    
    final_reach = np.mean(eval_reached) * 100
    final_reward = np.mean(eval_rewards)
    
    results = {
        "name": network_name,
        "params": sum(p.numel() for p in network.parameters() if p.requires_grad),
        "training_time": elapsed,
        "final_reward": final_reward,
        "final_reach_rate": final_reach,
        "best_reach_rate": best_reach_rate * 100,
        "logs": logs,
    }
    
    print(f"\n  å®Œæˆ! è€—æ—¶: {elapsed:.1f}s")
    print(f"  è¯„ä¼°åˆ°è¾¾ç‡ (100 ep): {final_reach:.1f}%")
    print(f"  è¯„ä¼°å¥–åŠ±: {final_reward:.2f}")
    
    return results


def plot_comparison_v2(results: List[Dict], save_path: Path):
    """ç»˜åˆ¶å¯¹æ¯”å›¾ V2"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    window = 50
    
    def moving_avg(data, w):
        if len(data) < w:
            return data
        cumsum = np.cumsum(np.insert(data, 0, 0))
        return (cumsum[w:] - cumsum[:-w]) / w
    
    # 1. å¥–åŠ±æ›²çº¿
    ax = axes[0, 0]
    for i, r in enumerate(results):
        eps = r["logs"]["episode"]
        rewards = r["logs"]["reward"]
        ma = moving_avg(rewards, window)
        ax.plot(eps[window-1:], ma, color=colors[i % len(colors)], 
                label=f'{r["name"]}', linewidth=2)
    ax.set_xlabel("Episode")
    ax.set_ylabel("Reward (MA-50)")
    ax.set_title("Training Reward")
    ax.legend()
    ax.grid(alpha=0.3)
    
    # 2. åˆ°è¾¾ç‡æ›²çº¿
    ax = axes[0, 1]
    for i, r in enumerate(results):
        eps = r["logs"]["episode"]
        reached = np.array(r["logs"]["reached"]) * 100
        ma = moving_avg(reached, window)
        ax.plot(eps[window-1:], ma, color=colors[i % len(colors)], 
                label=r["name"], linewidth=2)
    ax.set_xlabel("Episode")
    ax.set_ylabel("Reach Rate (%)")
    ax.set_title("Target Reach Rate (MA-50)")
    ax.axhline(y=80, color='gray', linestyle='--', alpha=0.5)
    ax.set_ylim([0, 105])
    ax.legend()
    ax.grid(alpha=0.3)
    
    # 3. Îµ å’Œå­¦ä¹ ç‡æ›²çº¿
    ax = axes[1, 0]
    for i, r in enumerate(results):
        eps = r["logs"]["episode"]
        epsilon = r["logs"]["epsilon"]
        ax.plot(eps, epsilon, color=colors[i % len(colors)], 
                label=f'{r["name"]} Îµ', linewidth=2)
    ax.set_xlabel("Episode")
    ax.set_ylabel("Epsilon")
    ax.set_title("Exploration Rate (Îµ)")
    ax.legend()
    ax.grid(alpha=0.3)
    
    # 4. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
    ax = axes[1, 1]
    names = [r["name"] for r in results]
    final_reach = [r["final_reach_rate"] for r in results]
    best_reach = [r["best_reach_rate"] for r in results]
    
    x = np.arange(len(names))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, final_reach, width, label='Final (Eval)', color='#2ecc71')
    bars2 = ax.bar(x + width/2, best_reach, width, label='Best (Training)', color='#3498db', alpha=0.7)
    
    ax.set_ylabel('Reach Rate (%)')
    ax.set_title('Performance Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels([n.replace(' ', '\n') for n in names], fontsize=9)
    ax.legend()
    ax.set_ylim([0, 110])
    ax.grid(alpha=0.3, axis='y')
    
    for bar, val in zip(bars1, final_reach):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                f'{val:.1f}%', ha='center', va='bottom', fontsize=9)
    
    fig.suptitle('DQN Architecture Comparison V2 (with Double DQN + Soft Update)', 
                 fontsize=14, fontweight='bold')
    fig.tight_layout()
    
    save_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(save_path, dpi=150)
    plt.close(fig)
    print(f"\nä¿å­˜å¯¹æ¯”å›¾åˆ°: {save_path}")


def print_summary_v2(results: List[Dict]):
    """æ‰“å°æ±‡æ€»"""
    print("\n" + "=" * 90)
    print("æ€§èƒ½å¯¹æ¯”æ±‡æ€» (V2 - æ”¹è¿›è®­ç»ƒ)")
    print("=" * 90)
    print(f"{'ç½‘ç»œ':<20} {'å‚æ•°é‡':>12} {'æ—¶é—´':>8} {'æœ€ç»ˆåˆ°è¾¾ç‡':>12} {'æœ€ä½³åˆ°è¾¾ç‡':>12} {'æœ€ç»ˆå¥–åŠ±':>10}")
    print("-" * 90)
    
    for r in results:
        print(
            f"{r['name']:<20} "
            f"{r['params']:>12,} "
            f"{r['training_time']:>7.1f}s "
            f"{r['final_reach_rate']:>11.1f}% "
            f"{r['best_reach_rate']:>11.1f}% "
            f"{r['final_reward']:>10.2f}"
        )
    
    print("=" * 90)
    
    best = max(results, key=lambda x: x['final_reach_rate'])
    print(f"\nğŸ† æœ€ä½³ç½‘ç»œ: {best['name']} ({best['final_reach_rate']:.1f}%)")


def parse_args():
    p = argparse.ArgumentParser(description="DQN æ¶æ„å¯¹æ¯” V2")
    p.add_argument("--episodes", type=int, default=2000)
    p.add_argument("--device", type=str, default="cuda")
    p.add_argument("--output-dir", type=Path, default=Path("outputs/architecture_comparison_v2"))
    p.add_argument("--seed", type=int, default=42)
    p.add_argument("--quick", action="store_true")
    return p.parse_args()


def main():
    args = parse_args()
    
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    if args.quick:
        args.episodes = 1000
        log_interval = 100
    else:
        log_interval = 200
    
    # ç¯å¢ƒ
    env = RLArm2DEnv(RLArmConfig(max_steps=100, target_radius=0.1))
    state_dim = env.observation_dim
    action_dim = env.action_space_n
    
    print(f"\nç¯å¢ƒ: state_dim={state_dim}, action_dim={action_dim}")
    print(f"è®­ç»ƒ episodes: {args.episodes}")
    print(f"æ”¹è¿›: Double DQN, Soft Update, Linear Îµ decay, Cosine LR")
    
    # ç½‘ç»œ
    networks = [
        ("CNN+LSTM", DQNNetwork(state_dim, action_dim, seq_len=1)),
        ("LightTransformer", LightTransformerDQN(state_dim, action_dim, seq_len=1, d_model=64, n_heads=4)),
        ("Transformer", TransformerDQN(state_dim, action_dim, seq_len=1, d_model=64, n_heads=4, n_layers=2)),
    ]
    
    results = []
    for name, network in networks:
        network = network.to(device)
        
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        
        result = train_network_v2(
            network_name=name,
            network=network,
            env=env,
            num_episodes=args.episodes,
            device=device,
            log_interval=log_interval,
        )
        results.append(result)
    
    print_summary_v2(results)
    
    args.output_dir.mkdir(parents=True, exist_ok=True)
    plot_comparison_v2(results, args.output_dir / "comparison_v2.png")
    
    # ä¿å­˜ç»“æœ
    summary = {
        "config": {"episodes": args.episodes, "seed": args.seed, "version": "v2"},
        "improvements": ["Double DQN", "Soft Update", "Linear Îµ decay", "Cosine LR", "Best weights restore"],
        "results": [
            {k: v for k, v in r.items() if k != "logs"}
            for r in results
        ]
    }
    with open(args.output_dir / "summary_v2.json", "w") as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nç»“æœä¿å­˜åˆ°: {args.output_dir}")


if __name__ == "__main__":
    main()

