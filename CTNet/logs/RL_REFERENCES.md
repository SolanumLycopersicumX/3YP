# RL æ¨¡å—æŠ€æœ¯æ–‡çŒ®å¼•ç”¨ (Technical References)

> æœ¬æ–‡ä»¶è®°å½• RL ä»£ç ä¸­ä½¿ç”¨çš„å„é¡¹æŠ€æœ¯åŠå…¶å­¦æœ¯æ–‡çŒ®æ¥æºã€‚
> 
> æœ€åæ›´æ–°ï¼š2026-02-02

---

## ç›®å½•

1. [å¼ºåŒ–å­¦ä¹ åŸºç¡€](#1-å¼ºåŒ–å­¦ä¹ åŸºç¡€-reinforcement-learning-fundamentals)
2. [æ·±åº¦ Q ç½‘ç»œ](#2-æ·±åº¦-q-ç½‘ç»œ-deep-q-network)
3. [CNN ç‰¹å¾æå–](#3-cnn-ç‰¹å¾æå–)
4. [LSTM åºåˆ—å»ºæ¨¡](#4-lstm-åºåˆ—å»ºæ¨¡)
5. [Transformer æ¶æ„](#5-transformer-æ¶æ„)
6. [è®­ç»ƒæŠ€å·§](#6-è®­ç»ƒæŠ€å·§)
7. [ä»£ç -æ–‡çŒ®æ˜ å°„è¡¨](#7-ä»£ç -æ–‡çŒ®æ˜ å°„è¡¨)

---

## 1. å¼ºåŒ–å­¦ä¹ åŸºç¡€ (Reinforcement Learning Fundamentals)

### Q-Learning

**ç†è®ºåŸºç¡€ï¼š**
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max_a' Q(s',a') - Q(s,a)]
```

**æ–‡çŒ®ï¼š**
> **[1] Watkins, C.J.C.H., & Dayan, P. (1992)**
> "Q-learning"
> *Machine Learning, 8(3-4), 279-292*
> 
> ğŸ“Œ **è´¡çŒ®**: æå‡º Q-Learning ç®—æ³•ï¼Œå¥ å®šäº†å€¼å‡½æ•°å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_model.py` - `dqn_training_step()`

---

### ç»éªŒå›æ”¾ (Experience Replay)

**åŸç†ï¼š** å°†ç»éªŒ (s, a, r, s', done) å­˜å‚¨åœ¨ç¼“å†²åŒºï¼Œéšæœºé‡‡æ ·è®­ç»ƒï¼Œæ‰“ç ´æ•°æ®ç›¸å…³æ€§

**æ–‡çŒ®ï¼š**
> **[2] Lin, L.J. (1992)**
> "Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching"
> *Machine Learning, 8(3-4), 293-321*
> 
> ğŸ“Œ **è´¡çŒ®**: é¦–æ¬¡æå‡ºç»éªŒå›æ”¾æœºåˆ¶

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_model.py` - `class ReplayBuffer`

---

### Îµ-Greedy æ¢ç´¢ç­–ç•¥

**åŸç†ï¼š**
```python
if random() < Îµ:
    action = random_action()  # æ¢ç´¢
else:
    action = argmax(Q(s))     # åˆ©ç”¨
```

**æ–‡çŒ®ï¼š**
> **[3] Sutton, R.S., & Barto, A.G. (2018)**
> "Reinforcement Learning: An Introduction" (2nd Edition)
> *MIT Press*
> 
> ğŸ“Œ **è´¡çŒ®**: å¼ºåŒ–å­¦ä¹ ç»å…¸æ•™æï¼Œç³»ç»Ÿä»‹ç»æ¢ç´¢-åˆ©ç”¨æƒè¡¡

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_model.py` - `epsilon_greedy_action()`

---

## 2. æ·±åº¦ Q ç½‘ç»œ (Deep Q-Network)

### DQN (Deep Q-Network)

**æ ¸å¿ƒåˆ›æ–°ï¼š**
1. ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿‘ä¼¼ Q å‡½æ•°
2. ç»éªŒå›æ”¾
3. ç›®æ ‡ç½‘ç»œ (Target Network)

**æ–‡çŒ®ï¼š**
> **[4] Mnih, V., et al. (2013)**
> "Playing Atari with Deep Reinforcement Learning"
> *arXiv preprint arXiv:1312.5602*
> 
> ğŸ“Œ **è´¡çŒ®**: é¦–æ¬¡å°†æ·±åº¦å­¦ä¹ ä¸ Q-Learning ç»“åˆ

> **[5] Mnih, V., et al. (2015)**
> "Human-level control through deep reinforcement learning"
> *Nature, 518(7540), 529-533*
> 
> ğŸ“Œ **è´¡çŒ®**: DQN çš„æ­£å¼ç‰ˆæœ¬ï¼Œå‘è¡¨äº Natureï¼Œå±•ç¤ºè¶…äººç±»æ°´å¹³çš„ Atari æ¸¸æˆè¡¨ç°

**ä»£ç ä½ç½®ï¼š** 
- `scripts/dqn_model.py` - `class DQNNetwork`
- `scripts/train_dqn_rl.py` - è®­ç»ƒå¾ªç¯

---

### Double DQN

**é—®é¢˜ï¼š** æ ‡å‡† DQN ä¼šè¿‡é«˜ä¼°è®¡ Q å€¼

**è§£å†³æ–¹æ¡ˆï¼š** åˆ†ç¦»åŠ¨ä½œé€‰æ‹©å’ŒåŠ¨ä½œè¯„ä¼°
```python
# æ ‡å‡† DQN
next_q = target_net(s').max()

# Double DQN
a* = argmax(policy_net(s'))  # Policy é€‰åŠ¨ä½œ
next_q = target_net(s')[a*]   # Target è¯„ä¼°
```

**æ–‡çŒ®ï¼š**
> **[6] Van Hasselt, H., Guez, A., & Silver, D. (2016)**
> "Deep Reinforcement Learning with Double Q-learning"
> *Proceedings of the AAAI Conference on Artificial Intelligence, 30(1)*
> 
> ğŸ“Œ **è´¡çŒ®**: è§£å†³ Q å€¼è¿‡ä¼°è®¡é—®é¢˜ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§

**ä»£ç ä½ç½®ï¼š** `scripts/compare_dqn_v2.py` - `double_dqn_training_step()`

---

### Target Network è½¯æ›´æ–° (Soft Update)

**åŸç†ï¼š** æ¸è¿›å¼æ›´æ–°ç›®æ ‡ç½‘ç»œ
```python
Î¸_target = Ï„Â·Î¸_policy + (1-Ï„)Â·Î¸_target  # Ï„ â‰ˆ 0.005
```

**æ–‡çŒ®ï¼š**
> **[7] Lillicrap, T.P., et al. (2015)**
> "Continuous control with deep reinforcement learning"
> *arXiv preprint arXiv:1509.02971* (DDPG)
> 
> ğŸ“Œ **è´¡çŒ®**: æå‡ºè½¯æ›´æ–°æœºåˆ¶ï¼Œä½¿è®­ç»ƒæ›´åŠ å¹³æ»‘

**ä»£ç ä½ç½®ï¼š** `scripts/compare_dqn_v2.py` - è®­ç»ƒå¾ªç¯ä¸­çš„è½¯æ›´æ–°éƒ¨åˆ†

---

### Huber Loss (Smooth L1 Loss)

**åŸç†ï¼š** ç»“åˆ L1 å’Œ L2 æŸå¤±çš„ä¼˜ç‚¹
```
L(Î´) = 0.5Â·Î´Â²           if |Î´| â‰¤ 1
     = |Î´| - 0.5        otherwise
```

**æ–‡çŒ®ï¼š**
> **[8] Huber, P.J. (1964)**
> "Robust Estimation of a Location Parameter"
> *Annals of Mathematical Statistics, 35(1), 73-101*
> 
> ğŸ“Œ **è´¡çŒ®**: å¯¹å¼‚å¸¸å€¼æ›´é²æ£’çš„æŸå¤±å‡½æ•°

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_model.py` - `F.smooth_l1_loss()`

---

## 3. CNN ç‰¹å¾æå–

### 1D å·ç§¯ç¥ç»ç½‘ç»œ

**åŸç†ï¼š** åœ¨æ—¶é—´/åºåˆ—ç»´åº¦ä¸Šè¿›è¡Œå·ç§¯ï¼Œæå–å±€éƒ¨ç‰¹å¾

**æ–‡çŒ®ï¼š**
> **[9] LeCun, Y., et al. (1998)**
> "Gradient-based learning applied to document recognition"
> *Proceedings of the IEEE, 86(11), 2278-2324*
> 
> ğŸ“Œ **è´¡çŒ®**: CNN çš„å¥ åŸºæ€§å·¥ä½œï¼ˆè™½ç„¶æ˜¯ 2Dï¼Œä½† 1D å·ç§¯æ˜¯è‡ªç„¶æ‰©å±•ï¼‰

> **[10] Kiranyaz, S., et al. (2021)**
> "1D Convolutional Neural Networks and Applications: A Survey"
> *Mechanical Systems and Signal Processing, 151, 107398*
> 
> ğŸ“Œ **è´¡çŒ®**: 1D CNN åœ¨æ—¶é—´åºåˆ—å’Œä¿¡å·å¤„ç†ä¸­çš„åº”ç”¨ç»¼è¿°

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_model.py` - `DQNNetwork.conv`

---

## 4. LSTM åºåˆ—å»ºæ¨¡

### Long Short-Term Memory

**æ ¸å¿ƒç»“æ„ï¼š**
- é—å¿˜é—¨ (Forget Gate)
- è¾“å…¥é—¨ (Input Gate)
- è¾“å‡ºé—¨ (Output Gate)
- ç»†èƒçŠ¶æ€ (Cell State)

**æ–‡çŒ®ï¼š**
> **[11] Hochreiter, S., & Schmidhuber, J. (1997)**
> "Long Short-Term Memory"
> *Neural Computation, 9(8), 1735-1780*
> 
> ğŸ“Œ **è´¡çŒ®**: æå‡º LSTMï¼Œè§£å†³ RNN çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_model.py` - `DQNNetwork.lstm`

---

## 5. Transformer æ¶æ„

### è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Self-Attention)

**æ ¸å¿ƒå…¬å¼ï¼š**
```
Attention(Q,K,V) = softmax(QK^T / âˆšd_k) V
```

**æ–‡çŒ®ï¼š**
> **[12] Vaswani, A., et al. (2017)**
> "Attention Is All You Need"
> *Advances in Neural Information Processing Systems (NeurIPS), 30*
> 
> ğŸ“Œ **è´¡çŒ®**: æå‡º Transformer æ¶æ„ï¼Œé©å‘½æ€§åœ°æ”¹å˜äº† NLP å’Œå…¶ä»–é¢†åŸŸ

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_transformer.py` - `TransformerDQN`, `LightTransformerDQN`

---

### ä½ç½®ç¼–ç  (Positional Encoding)

**æ­£å¼¦ä½ç½®ç¼–ç ï¼š**
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**æ–‡çŒ®ï¼š** åŒä¸Š [12] Vaswani et al., 2017

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_transformer.py` - `SinusoidalPositionalEncoding`

---

### Pre-LayerNorm Transformer

**åŸç†ï¼š** åœ¨æ³¨æ„åŠ›/FFN ä¹‹å‰åº”ç”¨ LayerNormï¼Œè®­ç»ƒæ›´ç¨³å®š

**æ–‡çŒ®ï¼š**
> **[13] Xiong, R., et al. (2020)**
> "On Layer Normalization in the Transformer Architecture"
> *International Conference on Machine Learning (ICML)*
> 
> ğŸ“Œ **è´¡çŒ®**: åˆ†æ Pre-LN vs Post-LNï¼Œè¯æ˜ Pre-LN æ›´ç¨³å®š

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_transformer.py` - `norm_first=True`

---

### Decision Transformer

**æ ¸å¿ƒæ€æƒ³ï¼š** å°† RL è½¬åŒ–ä¸ºåºåˆ—å»ºæ¨¡é—®é¢˜
```
[RÌ‚â‚, sâ‚, aâ‚, RÌ‚â‚‚, sâ‚‚, aâ‚‚, ...] â†’ Transformer â†’ Ã¢_t
```

**æ–‡çŒ®ï¼š**
> **[14] Chen, L., et al. (2021)**
> "Decision Transformer: Reinforcement Learning via Sequence Modeling"
> *Advances in Neural Information Processing Systems (NeurIPS), 34*
> 
> ğŸ“Œ **è´¡çŒ®**: å°† Transformer ç”¨äºç¦»çº¿ RLï¼Œä¸éœ€è¦ Bellman æ–¹ç¨‹

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_transformer.py` - `DecisionTransformerDQN`

---

## 6. è®­ç»ƒæŠ€å·§

### æ¢¯åº¦è£å‰ª (Gradient Clipping)

**æ–‡çŒ®ï¼š**
> **[15] Pascanu, R., Mikolov, T., & Bengio, Y. (2013)**
> "On the difficulty of training recurrent neural networks"
> *International Conference on Machine Learning (ICML)*
> 
> ğŸ“Œ **è´¡çŒ®**: åˆ†ææ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œæå‡ºæ¢¯åº¦è£å‰ª

**ä»£ç ä½ç½®ï¼š** `scripts/dqn_model.py` - `nn.utils.clip_grad_norm_()`

---

### Cosine Annealing å­¦ä¹ ç‡è°ƒåº¦

**æ–‡çŒ®ï¼š**
> **[16] Loshchilov, I., & Hutter, F. (2017)**
> "SGDR: Stochastic Gradient Descent with Warm Restarts"
> *International Conference on Learning Representations (ICLR)*
> 
> ğŸ“Œ **è´¡çŒ®**: æå‡º Cosine Annealing å­¦ä¹ ç‡è°ƒåº¦

**ä»£ç ä½ç½®ï¼š** `scripts/compare_dqn_v2.py` - `CosineAnnealingLR`

---

### AdamW ä¼˜åŒ–å™¨

**æ–‡çŒ®ï¼š**
> **[17] Loshchilov, I., & Hutter, F. (2019)**
> "Decoupled Weight Decay Regularization"
> *International Conference on Learning Representations (ICLR)*
> 
> ğŸ“Œ **è´¡çŒ®**: ä¿®æ­£ Adam ä¸­çš„æƒé‡è¡°å‡å®ç°

**ä»£ç ä½ç½®ï¼š** `scripts/compare_dqn_v2.py` - `torch.optim.AdamW`

---

## 7. ä»£ç -æ–‡çŒ®æ˜ å°„è¡¨

| ä»£ç æ–‡ä»¶ | æŠ€æœ¯ç»„ä»¶ | ä¸»è¦æ–‡çŒ® |
|----------|----------|----------|
| `dqn_model.py` | DQNNetwork (CNN+LSTM) | [5], [9], [11] |
| `dqn_model.py` | ReplayBuffer | [2] |
| `dqn_model.py` | epsilon_greedy_action | [3] |
| `dqn_model.py` | dqn_training_step | [5], [8] |
| `dqn_transformer.py` | TransformerDQN | [12], [13] |
| `dqn_transformer.py` | LightTransformerDQN | [12] |
| `dqn_transformer.py` | DecisionTransformerDQN | [14] |
| `dqn_transformer.py` | SinusoidalPositionalEncoding | [12] |
| `compare_dqn_v2.py` | Double DQN | [6] |
| `compare_dqn_v2.py` | Soft Update | [7] |
| `compare_dqn_v2.py` | Cosine LR | [16] |
| `compare_dqn_v2.py` | AdamW | [17] |
| `train_dqn_rl.py` | RLArm2DEnv (Reward Shaping) | [3] |

---

## å®Œæ•´å‚è€ƒæ–‡çŒ®åˆ—è¡¨

```bibtex
@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992}
}

@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={293--321},
  year={1992}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and others},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and others},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{huber1964robust,
  title={Robust estimation of a location parameter},
  author={Huber, Peter J},
  journal={Annals of mathematical statistics},
  volume={35},
  number={1},
  pages={73--101},
  year={1964}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998}
}

@article{kiranyaz20211d,
  title={1D convolutional neural networks and applications: A survey},
  author={Kiranyaz, Serkan and others},
  journal={Mechanical systems and signal processing},
  volume={151},
  pages={107398},
  year={2021}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and others},
  booktitle={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and others},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020}
}

@inproceedings{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and others},
  booktitle={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013}
}

@inproceedings{loshchilov2017sgdr,
  title={SGDR: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{loshchilov2019decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
```

---

*æœ¬æ–‡æ¡£ç”¨äºå­¦æœ¯å†™ä½œæ—¶çš„å¼•ç”¨å‚è€ƒã€‚*



