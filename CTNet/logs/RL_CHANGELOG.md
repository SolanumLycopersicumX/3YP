# RL æ¨¡å—å˜æ›´æ—¥å¿— (Reinforcement Learning Changelog)

> æœ¬æ–‡ä»¶è®°å½•æ‰€æœ‰ RL ç›¸å…³ä»£ç çš„ä¿®æ”¹å†å²ã€åŸå› ã€æ€è·¯å’Œæ€§èƒ½ç»“æœã€‚
> 
> æœ€åæ›´æ–°ï¼š2026-02-17

---

## âš ï¸ å˜æ›´è§„èŒƒ

**æ¯æ¬¡ä»£ç ä¿®æ”¹å¿…é¡»éµå¾ªä»¥ä¸‹æµç¨‹ï¼š**

1. **è®°å½•ä¿®æ”¹åŸå› ** - ä¸ºä»€ä¹ˆè¦åšè¿™ä¸ªä¿®æ”¹
2. **è®°å½•ä¿®æ”¹æ€è·¯** - æŠ€æœ¯æ–¹æ¡ˆå’Œå®ç°ç»†èŠ‚
3. **è®°å½•ä¿®æ”¹å†…å®¹** - å…·ä½“æ”¹åŠ¨äº†å“ªäº›ä»£ç 
4. **è¿è¡Œæµ‹è¯•å®éªŒ** - æ‰§è¡Œå¯¹æ¯”å®éªŒéªŒè¯æ•ˆæœ
5. **ç”Ÿæˆå¯è§†åŒ–ç»“æœ** - å¿…é¡»åŒ…å«è®­ç»ƒæ›²çº¿å›¾
6. **è®°å½•æ€§èƒ½è¡¨ç°** - é‡åŒ–æŒ‡æ ‡å¯¹æ¯”

**å¯è§†åŒ–è¦æ±‚ï¼š**
- è®­ç»ƒå¥–åŠ±æ›²çº¿ (Reward vs Episode)
- åˆ°è¾¾ç‡æ›²çº¿ (Reach Rate vs Episode)
- æŸå¤±æ›²çº¿ (Loss vs Episode)
- æ¢ç´¢ç‡æ›²çº¿ (Epsilon vs Episode)
- æœ€ç»ˆæ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
- æ‰€æœ‰å›¾è¡¨ä¿å­˜åˆ° `outputs/` ç›®å½•

---

## ç›®å½•

1. [ç‰ˆæœ¬æ¦‚è§ˆ](#ç‰ˆæœ¬æ¦‚è§ˆ)
2. [V1.0 - åŸºç¡€ DQN æ¡†æ¶](#v10---åŸºç¡€-dqn-æ¡†æ¶)
3. [V1.1 - Transformer ç½‘ç»œæ¶æ„](#v11---transformer-ç½‘ç»œæ¶æ„)
4. [V2.0 - è®­ç»ƒç¨³å®šæ€§æ”¹è¿›](#v20---è®­ç»ƒç¨³å®šæ€§æ”¹è¿›)
5. [V2.1 - Position vs Time å¯è§†åŒ–](#v21---position-vs-time-å¯è§†åŒ–)
6. [V2.2 - 4æ–¹å‘RLæ¨¡å‹ + ç‰©ç†æœºæ¢°è‡‚æ§åˆ¶](#v22---4æ–¹å‘rlæ¨¡å‹--ç‰©ç†æœºæ¢°è‡‚æ§åˆ¶)
7. [V2.3 - å¹³æ»‘æ§åˆ¶ + é™ä½ä¿æŠ¤](#v23---å¹³æ»‘æ§åˆ¶--é™ä½ä¿æŠ¤)
8. [æ€§èƒ½å¯¹æ¯”æ±‡æ€»](#æ€§èƒ½å¯¹æ¯”æ±‡æ€»)
9. [æ–‡ä»¶ç»“æ„](#æ–‡ä»¶ç»“æ„)
10. [æœªæ¥è®¡åˆ’](#æœªæ¥è®¡åˆ’)

---

## ç‰ˆæœ¬æ¦‚è§ˆ

| ç‰ˆæœ¬ | æ—¥æœŸ | ä¸»è¦å˜æ›´ | æœ€ä½³åˆ°è¾¾ç‡ |
|------|------|----------|------------|
| V1.0 | 2026-02-02 | åŸºç¡€ DQN + CNN+LSTM | 100% (5000 ep) |
| V1.1 | 2026-02-02 | æ–°å¢ Transformer æ¶æ„ | - |
| V2.0 | 2026-02-02 | Double DQN + Soft Update | 100% (1000 ep) |
| V2.1 | 2026-02-17 | Position vs Time å¯è§†åŒ– | - |
| V2.2 | 2026-02-17 | 4æ–¹å‘RLæ¨¡å‹ + ç‰©ç†æœºæ¢°è‡‚æ§åˆ¶ | 100% (ä»¿çœŸ+ç‰©ç†) |
| V2.3 | 2026-02-17 | å¹³æ»‘æ§åˆ¶ + é™ä½ä¿æŠ¤ | - |

---

## V1.0 - åŸºç¡€ DQN æ¡†æ¶

### åˆ›å»ºæ—¥æœŸ
2026-02-02

### åˆ›å»ºåŸå› 
- æ•™æˆä»»åŠ¡ 1ï¼šå°† CNN åˆ†ç±»å™¨æ›¿æ¢ä¸º RL æ§åˆ¶å™¨
- éœ€è¦ä¸€ä¸ªå®Œæ•´çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶

### æ–°å¢æ–‡ä»¶

| æ–‡ä»¶ | åŠŸèƒ½ |
|------|------|
| `scripts/dqn_model.py` | DQN ç½‘ç»œå®šä¹‰ (CNN+LSTM) |
| `scripts/train_dqn_rl.py` | RL è®­ç»ƒå¾ªç¯ + ç¯å¢ƒå®šä¹‰ |
| `scripts/test_rl_training.py` | å¿«é€ŸéªŒè¯æµ‹è¯• |
| `scripts/plot_dqn_training.py` | è®­ç»ƒæ›²çº¿å¯è§†åŒ– |

### æ ¸å¿ƒæ¶æ„

```
DQNNetwork (CNN+LSTM):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: (batch, seq_len=1, state_dim=5)  â”‚
â”‚           â†“                              â”‚
â”‚ Conv1D(5â†’64) â†’ ReLU â†’ Conv1D(64â†’64)     â”‚
â”‚           â†“                              â”‚
â”‚ LSTM(64â†’128)                             â”‚
â”‚           â†“                              â”‚
â”‚ FC(128â†’128) â†’ ReLU â†’ FC(128â†’4)          â”‚
â”‚           â†“                              â”‚
â”‚ Output: Q-values [4]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
å‚æ•°é‡: 129,732
```

### ç¯å¢ƒè®¾è®¡ (RLArm2DEnv)

**çŠ¶æ€ç©ºé—´ (5ç»´):**
```
obs = [y, z, target_y, target_z, distance_to_target]
```

**åŠ¨ä½œç©ºé—´ (4ä¸ªç¦»æ•£åŠ¨ä½œ):**
```
0: left  (+Y)
1: right (-Y)
2: up    (+Z)
3: down  (-Z)
```

**Reward è®¾è®¡:**
```python
reward = 0.0
reward += -0.01                           # æ¯æ­¥æƒ©ç½š
reward += (prev_dist - curr_dist) * 1.0   # è·ç¦»æ”¹å–„å¥–åŠ±
reward += 10.0 if reached else 0.0        # åˆ°è¾¾ç›®æ ‡å¥–åŠ±
reward += -0.5 if hit_boundary else 0.0   # è¾¹ç•Œæƒ©ç½š
reward += -0.5 if oscillation else 0.0    # æŒ¯è¡æƒ©ç½š
```

### è®­ç»ƒé…ç½® (V1.0)

| å‚æ•° | å€¼ |
|------|-----|
| Episodes | 5000 |
| Batch Size | 64 |
| Learning Rate | 1e-3 |
| Î³ (discount) | 0.99 |
| Îµ start | 1.0 |
| Îµ end | 0.05 |
| Îµ decay | 0.995 (æŒ‡æ•°è¡°å‡) |
| Buffer Size | 100,000 |
| Target Update | æ¯ 10 episode ç¡¬æ›´æ–° |

### è®­ç»ƒç»“æœ (5000 episodes)

| é˜¶æ®µ | Episodes | å¹³å‡å¥–åŠ± | åˆ°è¾¾ç‡ | å¹³å‡æ­¥æ•° |
|------|----------|----------|--------|----------|
| å‰æœŸ | 1-100 | 3.24 | 56% | 67 |
| ä¸­æœŸ | 101-500 | 3.26 | 63% | 60 |
| åæœŸ | 501-5000 | 9.93 | 99% | 16 |
| **æœ€ç»ˆ** | æœ€å100 | **10.32** | **100%** | **14.6** |

### ä¿å­˜çš„æ¨¡å‹
- `outputs/dqn_policy_full.pth` (5000 episodes è®­ç»ƒ)

### å¯è§†åŒ–ç»“æœ
- **è®­ç»ƒæ›²çº¿å›¾**: `outputs/dqn_training_curve.png`

![V1.0 è®­ç»ƒæ›²çº¿](../outputs/dqn_training_curve.png)

**æ›²çº¿è¯´æ˜ï¼š**
- å·¦ä¸Šï¼šå¥–åŠ±æ›²çº¿ç¨³å®šæ”¶æ•›åˆ° ~10
- å³ä¸Šï¼šåˆ°è¾¾ç‡ä» 0% ä¸Šå‡åˆ° 100%
- å·¦ä¸‹ï¼šæŸå¤±ä» ~0.07 ä¸‹é™åˆ° ~0.03
- å³ä¸‹ï¼šÎµ ä» 1.0 è¡°å‡åˆ° 0.05

---

## V1.1 - Transformer ç½‘ç»œæ¶æ„

### åˆ›å»ºæ—¥æœŸ
2026-02-02

### åˆ›å»ºåŸå› 
- æ•™æˆä»»åŠ¡ 5ï¼šæ¢ç´¢æ–°çš„ç½‘ç»œæ¶æ„
- Transformer åœ¨åºåˆ—å»ºæ¨¡ä¸Šæœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
- æ–‡çŒ®è¡¨æ˜ Transformer åœ¨ RL ä¸­æœ‰æ˜¾è‘—å¢ç›Š (Decision Transformer, 2021)

### æ–°å¢æ–‡ä»¶

| æ–‡ä»¶ | åŠŸèƒ½ |
|------|------|
| `scripts/dqn_transformer.py` | Transformer DQN ç½‘ç»œå®šä¹‰ |
| `scripts/compare_dqn_architectures.py` | æ¶æ„å¯¹æ¯”å®éªŒ V1 |

### æ–°å¢æ¶æ„

#### 1. LightTransformerDQN (è½»é‡çº§)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: (batch, seq_len=1, state_dim=5)  â”‚
â”‚           â†“                              â”‚
â”‚ Linear(5â†’32)                             â”‚
â”‚           â†“                              â”‚
â”‚ Self-Attention (2 heads) + Residual     â”‚
â”‚           â†“                              â”‚
â”‚ FFN(32â†’128â†’32) + Residual               â”‚
â”‚           â†“                              â”‚
â”‚ Linear(32â†’4)                             â”‚
â”‚           â†“                              â”‚
â”‚ Output: Q-values [4]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
å‚æ•°é‡: 13,028
```

#### 2. TransformerDQN (æ ‡å‡†ç‰ˆ)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: (batch, seq_len=1, state_dim=5)  â”‚
â”‚           â†“                              â”‚
â”‚ Linear(5â†’64)                             â”‚
â”‚           â†“                              â”‚
â”‚ Positional Encoding (Sinusoidal)        â”‚
â”‚           â†“                              â”‚
â”‚ â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
â”‚ â•‘ Transformer Encoder Ã— 2           â•‘   â”‚
â”‚ â•‘   - Multi-Head Attention (4 heads)â•‘   â”‚
â”‚ â•‘   - FFN (64â†’256â†’64)               â•‘   â”‚
â”‚ â•‘   - Pre-LN + Residual             â•‘   â”‚
â”‚ â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚           â†“                              â”‚
â”‚ Output Head: LN â†’ FC â†’ GELU â†’ FC        â”‚
â”‚           â†“                              â”‚
â”‚ Output: Q-values [4]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
å‚æ•°é‡: 104,900
```

#### 3. DecisionTransformerDQN (é«˜çº§ç‰ˆ)

```
ç‰¹ç‚¹ï¼š
- æ”¯æŒå†å²è½¨è¿¹è¾“å…¥ [sâ‚,aâ‚,râ‚, sâ‚‚,aâ‚‚,râ‚‚, ...]
- Token ç±»å‹åµŒå…¥ (state/action/return)
- å¯å­¦ä¹ ä½ç½®ç¼–ç 
å‚æ•°é‡: 605,188
```

### å¯¹æ¯”å®éªŒ V1 ç»“æœ (500 episodes)

**é—®é¢˜å‘ç°ï¼šæ‰€æœ‰æ¨¡å‹è®­ç»ƒåæœŸå´©æºƒï¼**

| ç½‘ç»œ | å‚æ•°é‡ | å³°å€¼è¡¨ç° | æœ€ç»ˆè¡¨ç° | é—®é¢˜ |
|------|--------|----------|----------|------|
| CNN+LSTM | 129,732 | ~95% @ ep150 | 31% | æ€§èƒ½ä¸‹é™ |
| LightTransformer | 13,028 | ~98% @ ep150 | 6% | ä¸¥é‡å´©æºƒ |
| Transformer | 104,900 | ~100% @ ep250 | 8% | ä¸¥é‡å´©æºƒ |

### å¯è§†åŒ–ç»“æœ
- **å¯¹æ¯”å›¾**: `outputs/architecture_comparison/comparison.png`

![V1.1 æ¶æ„å¯¹æ¯”](../outputs/architecture_comparison/comparison.png)

**æ›²çº¿è¯´æ˜ï¼š**
- å·¦ä¸Šï¼šä¸‰ç§ç½‘ç»œçš„å¥–åŠ±æ›²çº¿ï¼Œ**å‡åœ¨ ep250 åå´©æºƒ**
- å³ä¸Šï¼šåˆ°è¾¾ç‡æ›²çº¿ï¼Œå³°å€¼åæ€¥å‰§ä¸‹é™
- å·¦ä¸‹ï¼šæŸå¤±æ›²çº¿æŒç»­ä¸Šå‡ï¼ˆä¸ç¨³å®šä¿¡å·ï¼‰
- å³ä¸‹ï¼šæœ€ç»ˆæ€§èƒ½å¯¹æ¯”ï¼Œå‡ä½äº 35%

### é—®é¢˜åˆ†æ

| é—®é¢˜ | è§£é‡Š | å½±å“ |
|------|------|------|
| **1. Îµ è¡°å‡è¿‡å¿«** | 0.995^500 = 0.08ï¼Œæ¢ç´¢ç‡è¡°å‡å¤ªå¿« | æ¨¡å‹åœæ­¢æ¢ç´¢æ–°ç­–ç•¥ |
| **2. Q å€¼è¿‡ä¼°è®¡** | æ ‡å‡† DQN ä¼šé«˜ä¼° Q å€¼ | å¯¼è‡´æ¬¡ä¼˜åŠ¨ä½œè¢«é€‰æ‹© |
| **3. Target Network æ›´æ–°å¤ªé¢‘ç¹** | æ¯ 10 episode ç¡¬æ›´æ–° | è®­ç»ƒä¸ç¨³å®š |
| **4. ç»éªŒå›æ”¾åå·®** | åæœŸ buffer å……æ»¡æ—§ç»éªŒ | è¿‡æ‹Ÿåˆæ—©æœŸç­–ç•¥ |
| **5. Transformer æ›´æ•æ„Ÿ** | å‚æ•°é‡å¤§ï¼Œéœ€è¦æ›´ç»†è‡´è°ƒå‚ | å´©æºƒæ›´ä¸¥é‡ |

---

## V2.0 - è®­ç»ƒç¨³å®šæ€§æ”¹è¿›

### åˆ›å»ºæ—¥æœŸ
2026-02-02

### åˆ›å»ºåŸå› 
- è§£å†³ V1.1 ä¸­å‘ç°çš„è®­ç»ƒå´©æºƒé—®é¢˜
- æé«˜ Transformer ç½‘ç»œçš„è®­ç»ƒç¨³å®šæ€§

### æ–°å¢æ–‡ä»¶

| æ–‡ä»¶ | åŠŸèƒ½ |
|------|------|
| `scripts/compare_dqn_v2.py` | æ”¹è¿›ç‰ˆå¯¹æ¯”å®éªŒ |

### æ”¹è¿›å†…å®¹

#### æ”¹è¿› 1: Double DQNï¼ˆå‡å°‘ Q å€¼è¿‡ä¼°è®¡ï¼‰

**åŸç†ï¼š** å°†åŠ¨ä½œé€‰æ‹©å’Œ Q å€¼è¯„ä¼°åˆ†ç¦»

```python
# V1 æ ‡å‡† DQN (æœ‰é—®é¢˜)
next_q = target_net(next_states).max(1)[0]  # Target é€‰åŠ¨ä½œ + è¯„ä¼°

# V2 Double DQN (æ”¹è¿›)
next_actions = policy_net(next_states).argmax(dim=1)  # Policy é€‰åŠ¨ä½œ
next_q = target_net(next_states).gather(1, next_actions)  # Target è¯„ä¼°
```

#### æ”¹è¿› 2: Soft Updateï¼ˆæ›´å¹³æ»‘çš„ Target æ›´æ–°ï¼‰

**åŸç†ï¼š** æ¸è¿›å¼æ›´æ–° Target Network

```python
# V1 ç¡¬æ›´æ–° (æœ‰é—®é¢˜)
target_net.load_state_dict(policy_net.state_dict())  # å®Œå…¨å¤åˆ¶

# V2 è½¯æ›´æ–° (æ”¹è¿›)
Ï„ = 0.005  # è½¯æ›´æ–°ç³»æ•°
for param, target_param in zip(policy_net.parameters(), target_net.parameters()):
    target_param.data = Ï„ * param.data + (1 - Ï„) * target_param.data
```

#### æ”¹è¿› 3: æ›´æ…¢çš„ Îµ è¡°å‡

**åŸç†ï¼š** çº¿æ€§è¡°å‡æ›¿ä»£æŒ‡æ•°è¡°å‡ï¼Œä¿æŒæ›´é•¿çš„æ¢ç´¢æœŸ

```python
# V1 æŒ‡æ•°è¡°å‡ (å¤ªå¿«)
epsilon = 1.0 * (0.995 ** episode)  # 500 ep â†’ 0.08

# V2 çº¿æ€§è¡°å‡ (æ”¹è¿›)
epsilon = 1.0 - (episode / 1500) * (1.0 - 0.05)  # 1500 ep æ‰é™åˆ° 0.05
```

#### æ”¹è¿› 4: å­¦ä¹ ç‡è°ƒåº¦

**åŸç†ï¼š** Cosine Annealing å¹³æ»‘é™ä½å­¦ä¹ ç‡

```python
# V1 å›ºå®šå­¦ä¹ ç‡
optimizer = Adam(lr=1e-3)

# V2 Cosine è°ƒåº¦
optimizer = AdamW(lr=3e-4, weight_decay=1e-5)
scheduler = CosineAnnealingLR(optimizer, T_max=episodes, eta_min=lr*0.1)
```

#### æ”¹è¿› 5: æœ€ä½³æƒé‡æ¢å¤

**åŸç†ï¼š** ä¿å­˜å¹¶æ¢å¤è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³æ¨¡å‹

```python
# V2 æ–°å¢
if recent_reach_rate > best_reach_rate:
    best_weights = model.state_dict().clone()
    
# è®­ç»ƒç»“æŸåæ¢å¤æœ€ä½³æƒé‡
model.load_state_dict(best_weights)
```

### è®­ç»ƒé…ç½® (V2.0)

| å‚æ•° | V1 å€¼ | V2 å€¼ | å˜åŒ– |
|------|-------|-------|------|
| Learning Rate | 1e-3 | 3e-4 | â†“ é™ä½ |
| Îµ decay | 0.995 (æŒ‡æ•°) | çº¿æ€§ 1500ep | â†“ æ›´æ…¢ |
| Target Update | ç¡¬æ›´æ–°@10ep | Soft Ï„=0.005 | âœ“ æ›´å¹³æ»‘ |
| DQN Type | Standard | Double DQN | âœ“ å‡å°‘è¿‡ä¼°è®¡ |
| LR Schedule | None | Cosine | âœ“ æ–°å¢ |
| Weight Decay | 0 | 1e-5 | âœ“ æ–°å¢ |
| Best Weights | No | Yes | âœ“ æ–°å¢ |

### å¯¹æ¯”å®éªŒ V2 ç»“æœ (1000 episodes)

| ç½‘ç»œ | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | æœ€ç»ˆåˆ°è¾¾ç‡ | æœ€ä½³åˆ°è¾¾ç‡ | è¯„ä¼°å¥–åŠ± |
|------|--------|----------|------------|------------|----------|
| CNN+LSTM | 129,732 | 112s | 97% | 100% | 8.79 |
| LightTransformer | 50,628 | 134s | 99% | 99% | 10.07 |
| **Transformer** | 104,900 | 178s | **100%** | **100%** | **10.39** |

### V1 vs V2 æ€§èƒ½å¯¹æ¯”

| ç½‘ç»œ | V1 æœ€ç»ˆ (500ep) | V2 æœ€ç»ˆ (1000ep) | æ”¹å–„ |
|------|-----------------|------------------|------|
| CNN+LSTM | 31% âŒ | **97%** âœ… | +66% |
| LightTransformer | 6% âŒ | **99%** âœ… | +93% |
| Transformer | 8% âŒ | **100%** âœ… | +92% |

### å¯è§†åŒ–ç»“æœ
- **å¯¹æ¯”å›¾**: `outputs/architecture_comparison_v2/comparison_v2.png`

![V2.0 æ¶æ„å¯¹æ¯”](../outputs/architecture_comparison_v2/comparison_v2.png)

**æ›²çº¿è¯´æ˜ï¼š**
- å·¦ä¸Šï¼šä¸‰ç§ç½‘ç»œçš„å¥–åŠ±æ›²çº¿ï¼Œ**ç¨³å®šä¸Šå‡æ— å´©æºƒ**
- å³ä¸Šï¼šåˆ°è¾¾ç‡æ›²çº¿ï¼Œç¨³å®šæ”¶æ•›åˆ° ~100%
- å·¦ä¸‹ï¼šÎµ çº¿æ€§è¡°å‡æ›²çº¿ï¼Œæ¢ç´¢æ›´å……åˆ†
- å³ä¸‹ï¼šæœ€ç»ˆæ€§èƒ½å¯¹æ¯”ï¼ŒTransformer è¾¾åˆ° 100%

### å…³é”®å‘ç°

1. **Double DQN** æ˜¾è‘—å‡å°‘äº† Q å€¼è¿‡ä¼°è®¡é—®é¢˜
2. **Soft Update** ä½¿è®­ç»ƒæ›²çº¿æ›´åŠ å¹³æ»‘
3. **çº¿æ€§ Îµ è¡°å‡** ä¿è¯äº†å……åˆ†çš„æ¢ç´¢
4. **Cosine LR** åœ¨åæœŸç¨³å®šæ¨¡å‹
5. **æœ€ä½³æƒé‡æ¢å¤** ç¡®ä¿æœ€ç»ˆæ¨¡å‹æ˜¯æœ€ä¼˜çš„

---

## V2.1 - Position vs Time å¯è§†åŒ–

### åˆ›å»ºæ—¥æœŸ
2026-02-17

### åˆ›å»ºåŸå› 
- å¯¼å¸ˆåé¦ˆï¼šéœ€è¦ä¸€ä¸ª Position vs Time å›¾æ¥å±•ç¤ºç³»ç»Ÿçš„ overall performance
- é€šè¿‡ç›®æ ‡ä½ç½®ä¸å®é™…ä½ç½®çš„å¯¹æ¯”ï¼Œç›´è§‚å±•ç¤ºæ§åˆ¶ç²¾åº¦
- è¯¯å·®æ›²çº¿å¯ä»¥é‡åŒ–ç³»ç»Ÿçš„æ§åˆ¶æ€§èƒ½

### ä¿®æ”¹æ–‡ä»¶

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ |
|------|----------|
| `scripts/rl_control_test.py` | æ·»åŠ è½¨è¿¹è®°å½•å’Œå¯è§†åŒ–åŠŸèƒ½ |

### ä¿®æ”¹å†…å®¹

#### 1. æ–°å¢ `RLControlResult.sample_trajectory` å­—æ®µ

```python
@dataclass
class RLControlResult:
    # ... åŸæœ‰å­—æ®µ ...
    sample_trajectory: Optional[Dict[str, Any]] = None  # æ–°å¢
```

#### 2. ä¿®æ”¹ `run_rl_control_episode()` å‡½æ•°

**æ–°å¢å‚æ•°ï¼š** `record_trajectory: bool = False`

**æ–°å¢è®°å½•çš„æ•°æ®ï¼š**
- `time_steps`: æ—¶é—´æ­¥åˆ—è¡¨ [0, 1, 2, ...]
- `target_y`, `target_z`: ç›®æ ‡ä½ç½®ï¼ˆæ’å®šå€¼ï¼‰
- `actual_y`, `actual_z`: å®é™…ä½ç½®ï¼ˆéšæ—¶é—´å˜åŒ–ï¼‰
- `errors`: ä½ç½®è¯¯å·® = ||actual - target||

#### 3. æ–°å¢ `visualize_position_vs_time()` å‡½æ•°

**åŠŸèƒ½ï¼š** ç”Ÿæˆ Position vs Time å›¾

**è¾“å‡ºï¼š**
- å­å›¾ 1: Y ä½ç½®ï¼ˆå·¦å³æ–¹å‘ï¼‰
  - è“è‰²è™šçº¿ï¼šç›®æ ‡ Y ä½ç½®
  - çº¢è‰²å®çº¿ï¼šå®é™… Y ä½ç½®
  - æ©™è‰²åŒºåŸŸï¼šè¯¯å·®å¡«å……
- å­å›¾ 2: Z ä½ç½®ï¼ˆä¸Šä¸‹æ–¹å‘ï¼‰
  - åŒä¸Š
- å­å›¾ 3: ä½ç½®è¯¯å·®æ›²çº¿
  - ç»¿è‰²å®çº¿ï¼šè¯¯å·®éšæ—¶é—´å˜åŒ–
  - çº¢è‰²è™šçº¿ï¼šç›®æ ‡é˜ˆå€¼ (0.1)
  - æ˜¾ç¤º Final Error å’Œ Mean Error ç»Ÿè®¡

#### 4. æ–°å¢ `visualize_overall_performance()` å‡½æ•°

**åŠŸèƒ½ï¼š** ç”Ÿæˆç»¼åˆæ€§èƒ½å›¾

**è¾“å‡ºï¼š**
- å­å›¾ 1: å¹³å‡è¯¯å·® vs æœ€ç»ˆè¯¯å·®å¯¹æ¯”ï¼ˆå„æ•°æ®é›†ï¼‰
- å­å›¾ 2: è¯¯å·®æ”¶æ•›æ›²çº¿å¯¹æ¯”ï¼ˆå åŠ æ˜¾ç¤ºï¼‰
- å­å›¾ 3: ç»¼åˆæ€§èƒ½æŒ‡æ ‡ï¼ˆåˆ†ç±»å‡†ç¡®ç‡ã€åˆ°è¾¾ç‡ã€å¹³æ»‘åº¦ã€1-è¯¯å·®ï¼‰

### è¾“å‡ºæ–‡ä»¶

| æ–‡ä»¶ | æè¿° |
|------|------|
| `outputs/rl_control_test/position_vs_time.png` | Position vs Time å›¾ |
| `outputs/rl_control_test/overall_performance.png` | ç»¼åˆæ€§èƒ½å›¾ |

### å¯è§†åŒ–è®¾è®¡

```
Position vs Time å›¾:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Y Position    â”‚  â”‚ Z Position    â”‚  â”‚ Error Curve   â”‚   â”‚
â”‚  â”‚               â”‚  â”‚               â”‚  â”‚               â”‚   â”‚
â”‚  â”‚ --- Target    â”‚  â”‚ --- Target    â”‚  â”‚ â”€â”€â”€ Error     â”‚   â”‚
â”‚  â”‚ â”€â”€â”€ Actual    â”‚  â”‚ â”€â”€â”€ Actual    â”‚  â”‚ --- Threshold â”‚   â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆ Error     â”‚  â”‚ â–ˆâ–ˆâ–ˆ Error     â”‚  â”‚               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  Dataset: IV-2a / IV-2b / PhysioNet                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä½¿ç”¨æ–¹æ³•

```bash
# è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆå¯è§†åŒ–
python scripts/rl_control_test.py \
    --datasets IV-2a IV-2b PhysioNet \
    --subjects 1 2 3 \
    --model transformer \
    --device cuda

# è¾“å‡ºæ–‡ä»¶:
# - outputs/rl_control_test/position_vs_time.png
# - outputs/rl_control_test/overall_performance.png
# - outputs/rl_control_test/rl_control_comparison.png
# - outputs/rl_control_test/rl_control_comparison.json
```

---

## æ€§èƒ½å¯¹æ¯”æ±‡æ€»

### è®­ç»ƒæ›²çº¿å¯¹æ¯”

```
V1 (å´©æºƒ):
Episode:  0 â”€â”€â”€â”€â”€â”€ 150 â”€â”€â”€â”€â”€â”€ 300 â”€â”€â”€â”€â”€â”€ 500
Reach:    0% â†’ 98%â†‘ â†’ ä¸‹é™ â†’ 6%â†“ (å´©æºƒ!)

V2 (ç¨³å®š):
Episode:  0 â”€â”€â”€â”€â”€â”€ 300 â”€â”€â”€â”€â”€â”€ 600 â”€â”€â”€â”€â”€â”€ 1000
Reach:    0% â†’ 60%â†‘ â†’ 95%â†‘ â†’ 100%âœ“ (ç¨³å®š!)
```

### æœ€ç»ˆæ¨è

| åœºæ™¯ | æ¨èç½‘ç»œ | ç†ç”± |
|------|----------|------|
| ç®€å•ä»»åŠ¡/å¿«é€Ÿå®éªŒ | CNN+LSTM | è®­ç»ƒæœ€å¿« (112s) |
| éœ€è¦æœ€é«˜æ€§èƒ½ | Transformer | 100% åˆ°è¾¾ç‡ |
| èµ„æºæœ‰é™ | LightTransformer | å‚æ•°å°‘ (50K)ï¼Œæ•ˆæœå¥½ (99%) |

---

## æ–‡ä»¶ç»“æ„

```
scripts/
â”œâ”€â”€ dqn_model.py                    # V1.0 - CNN+LSTM ç½‘ç»œ
â”œâ”€â”€ dqn_transformer.py              # V1.1 - Transformer ç½‘ç»œ
â”œâ”€â”€ train_dqn_rl.py                 # V1.0 - è®­ç»ƒå¾ªç¯ + ç¯å¢ƒ
â”œâ”€â”€ train_rl_4direction.py          # V2.2 - 4æ–¹å‘RLè®­ç»ƒ
â”œâ”€â”€ rl_physical_control.py          # V2.2 - ç‰©ç†æœºæ¢°è‡‚æ§åˆ¶
â”œâ”€â”€ test_smooth_control.py          # V2.3 - å¹³æ»‘å‚æ•°æµ‹è¯• (æ–°å¢)
â”œâ”€â”€ test_rl_training.py             # V1.0 - å¿«é€Ÿæµ‹è¯•
â”œâ”€â”€ plot_dqn_training.py            # V1.0 - å¯è§†åŒ–
â”œâ”€â”€ compare_dqn_architectures.py    # V1.1 - å¯¹æ¯”å®éªŒ V1
â””â”€â”€ compare_dqn_v2.py               # V2.0 - å¯¹æ¯”å®éªŒ V2 (æ”¹è¿›ç‰ˆ)

# ç¯å¢ƒç±»
serial_arm_env.py                   # V1 - åŸºç¡€ä¸²å£ç¯å¢ƒ
serial_arm_env_v2.py                # V2.3 - ä¼˜åŒ–ç‰ˆä¸²å£ç¯å¢ƒ (æ–°å¢) â­

outputs/
â”œâ”€â”€ dqn_policy_full.pth             # V1.0 è®­ç»ƒæ¨¡å‹ (5000ep)
â”œâ”€â”€ dqn_policy_full.json            # V1.0 è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ dqn_training_curve.png          # V1.0 è®­ç»ƒæ›²çº¿
â”œâ”€â”€ rl_4direction_policy.pth        # V2.2 è®­ç»ƒæ¨¡å‹ (4æ–¹å‘) â­
â”œâ”€â”€ architecture_comparison/        # V1.1 å¯¹æ¯”ç»“æœ
â”‚   â”œâ”€â”€ comparison.png
â”‚   â””â”€â”€ summary.json
â”œâ”€â”€ architecture_comparison_v2/     # V2.0 å¯¹æ¯”ç»“æœ
â”‚   â”œâ”€â”€ comparison_v2.png
â”‚   â””â”€â”€ summary_v2.json
â””â”€â”€ rl_physical_control/            # V2.2 ç‰©ç†æ§åˆ¶ç»“æœ
    â””â”€â”€ results.json

logs/
â””â”€â”€ RL_CHANGELOG.md                 # æœ¬æ–‡ä»¶ - å˜æ›´æ—¥å¿—
```

---

## V2.2 - 4æ–¹å‘RLæ¨¡å‹ + ç‰©ç†æœºæ¢°è‡‚æ§åˆ¶

### åˆ›å»ºæ—¥æœŸ
2026-02-17

### åˆ›å»ºåŸå› 
- ä¹‹å‰çš„ `dqn_policy_full.pth` æ¨¡å‹åœ¨ç‰©ç†æœºæ¢°è‡‚æµ‹è¯•ä¸­åªä¼šæ‰§è¡Œ "down" åŠ¨ä½œ
- è¯¥æ¨¡å‹æœªèƒ½æ­£ç¡®å­¦ä¹  4 ä¸ªæ–¹å‘ (left, right, up, down) çš„æ§åˆ¶
- éœ€è¦é‡æ–°è®­ç»ƒä¸€ä¸ªä¸“é—¨é’ˆå¯¹ 4 æ–¹å‘æ§åˆ¶çš„ RL æ¨¡å‹

### é—®é¢˜åˆ†æ

| ç°è±¡ | åŸå›  | 
|------|------|
| RL åªæ‰§è¡Œ "down" åŠ¨ä½œ | æ—§æ¨¡å‹å¯èƒ½åœ¨ä¸åŒçš„ reward è®¾ç½®ä¸‹è®­ç»ƒ |
| ç‰©ç†æ§åˆ¶åˆ°è¾¾ç‡ 60% | åªæœ‰ç›®æ ‡åœ¨ "down" æ–¹å‘æ—¶æ‰èƒ½åˆ°è¾¾ |
| left/right æ–¹å‘å¤±è´¥ | æ¨¡å‹æ²¡æœ‰å­¦ä¹ åˆ°æ­£ç¡®çš„ Q å€¼åˆ†å¸ƒ |

### æ–°å¢æ–‡ä»¶

| æ–‡ä»¶ | åŠŸèƒ½ |
|------|------|
| `scripts/train_rl_4direction.py` | ä¸“é—¨è®­ç»ƒ 4 æ–¹å‘ RL æ¨¡å‹ |
| `scripts/rl_physical_control.py` | ç‰©ç†æœºæ¢°è‡‚æ§åˆ¶è„šæœ¬ |

### è®­ç»ƒé…ç½®

| å‚æ•° | å€¼ |
|------|-----|
| Model | TransformerDQN |
| Episodes | 1000 |
| Batch Size | 64 |
| Learning Rate | 3e-4 |
| Îµ start â†’ end | 1.0 â†’ 0.05 (çº¿æ€§è¡°å‡) |
| Target Update | Soft Update Ï„=0.005 |
| DQN Type | Double DQN |
| LR Schedule | Cosine Annealing |
| Best Weights | Yes |

### è®­ç»ƒç»“æœ (ä»¿çœŸç¯å¢ƒ)

```
[ 100/1000] reward= -0.97 reach= 19.0% Îµ=0.882 | L:21% R:20% U:20% D:20%
[ 200/1000] reward=  4.45 reach= 62.0% Îµ=0.764 | L:40% R:68% U:60% D:72%
[ 300/1000] reward=  6.66 reach= 79.0% Îµ=0.645 | L:84% R:80% U:72% D:84%
[ 400/1000] reward=  8.67 reach= 93.0% Îµ=0.526 | L:92% R:92% U:88% D:100%
[ 500/1000] reward=  9.45 reach= 98.0% Îµ=0.407 | L:96% R:100% U:96% D:100%
[ 600/1000] reward=  9.53 reach= 97.0% Îµ=0.289 | L:96% R:100% U:96% D:100%
[ 700/1000] reward=  9.99 reach= 99.0% Îµ=0.170 | L:100% R:100% U:96% D:100%
[ 800/1000] reward= 10.24 reach=100.0% Îµ=0.051 | L:100% R:100% U:100% D:100%
[ 900/1000] reward= 10.28 reach=100.0% Îµ=0.050 | L:100% R:100% U:100% D:100%
[1000/1000] reward= 10.28 reach=100.0% Îµ=0.050 | L:100% R:100% U:100% D:100%
```

### æœ€ç»ˆè¯„ä¼° (100 episodes, ä»¿çœŸ)

| æ–¹å‘ | åˆ°è¾¾ç‡ |
|------|--------|
| left | 100.0% (32/32) âœ… |
| right | 100.0% (20/20) âœ… |
| up | 100.0% (22/22) âœ… |
| down | 100.0% (26/26) âœ… |
| **Total** | **100.0%** ğŸ† |

è®­ç»ƒæ—¶é—´: 73.8s

### ç‰©ç†æœºæ¢°è‡‚æ§åˆ¶ç»“æœ

| æŒ‡æ ‡ | æ—§æ¨¡å‹ (dqn_policy_full.pth) | æ–°æ¨¡å‹ (rl_4direction_policy.pth) |
|------|------------------------------|-----------------------------------|
| åˆ†ç±»å‡†ç¡®ç‡ | 100% (5/5) | 100% (8/8) |
| æ§åˆ¶åˆ°è¾¾ç‡ | 60% (3/5) âŒ | **100% (8/8)** âœ… |
| å¹³å‡æ­¥æ•° | 12.8 | **8.2** |
| å¹³å‡å¥–åŠ± | 6.01 | **10.31** |
| left æˆåŠŸ | âŒ | âœ… |
| right æˆåŠŸ | âŒ | âœ… |
| up æˆåŠŸ | - | âœ… |
| down æˆåŠŸ | âœ… | âœ… |

### ä¿å­˜çš„æ¨¡å‹
- `outputs/rl_4direction_policy.pth` (4æ–¹å‘ Transformer DQN)

### å…³é”®ä¿®å¤

1. **PyTorch 2.6 æ¨¡å‹åŠ è½½é—®é¢˜**
   ```python
   # ä¿®å¤å‰: é»˜è®¤ weights_only=True å¯¼è‡´åŠ è½½å¤±è´¥
   state = torch.load(model_path, map_location=device)
   
   # ä¿®å¤å: æ˜¾å¼è®¾ç½® weights_only=False
   state = torch.load(model_path, map_location=device, weights_only=False)
   ```

2. **JSON åºåˆ—åŒ–é—®é¢˜**
   ```python
   # ä¿®å¤å‰: numpy.bool_ æ— æ³• JSON åºåˆ—åŒ–
   results['trials'].append({'reached': info.get('reached')})  # numpy.bool_
   
   # ä¿®å¤å: è½¬æ¢ä¸º Python bool
   for trial in results['trials']:
       if isinstance(trial['reached'], np.bool_):
           trial['reached'] = bool(trial['reached'])
   ```

3. **ä¸²å£æƒé™é—®é¢˜**
   ```bash
   # ä¸´æ—¶è§£å†³
   sudo chmod 666 /dev/ttyACM1
   
   # æ°¸ä¹…è§£å†³
   sudo usermod -aG dialout $USER
   ```

### ä½¿ç”¨æ–¹æ³•

```bash
# 1. è®­ç»ƒ 4 æ–¹å‘ RL æ¨¡å‹
python scripts/train_rl_4direction.py --model transformer --episodes 1000 --device cuda

# 2. æµ‹è¯•ç‰©ç†æœºæ¢°è‡‚
sudo chmod 666 /dev/ttyACM1  # æˆ–å·²åŠ å…¥ dialout ç»„åˆ™æ— éœ€æ­¤æ­¥
python scripts/rl_physical_control.py \
    --serial-port /dev/ttyACM1 \
    --subject 1 \
    --dataset A \
    --num-trials 8 \
    --rl-model transformer \
    --pre-home \
    --post-home
```

### è¾“å‡ºæ–‡ä»¶

| æ–‡ä»¶ | æè¿° |
|------|------|
| `outputs/rl_4direction_policy.pth` | è®­ç»ƒå¥½çš„ 4 æ–¹å‘ RL æ¨¡å‹ |
| `outputs/rl_physical_control/results.json` | ç‰©ç†æ§åˆ¶æµ‹è¯•ç»“æœ |

---

## V2.3 - å¹³æ»‘æ§åˆ¶ + é™ä½ä¿æŠ¤

### åˆ›å»ºæ—¥æœŸ
2026-02-17

### åˆ›å»ºåŸå› 
- ç‰©ç†æœºæ¢°è‡‚è¿åŠ¨æ—¶å­˜åœ¨"æŠ½æ/é¢¤æŠ–"ç°è±¡
- å¿«é€Ÿå¯åœå¯¼è‡´ä¼ºæœç”µæœºéœ‡åŠ¨
- ç´¯ç§¯å¤šæ­¥åå®¹æ˜“ç¢°è§¦ç‰©ç†é™ä½
- æ­¥é•¿è¿‡çŸ­å¯¼è‡´æ–¹å‘éš¾ä»¥åˆ†è¾¨

### é—®é¢˜åˆ†æ

| é—®é¢˜ | åŸå›  | å½±å“ |
|------|------|------|
| æŠ½æ/é¢¤æŠ– | ä¼ºæœå¿«é€Ÿå¯åœ | è¿åŠ¨ä¸å¹³æ»‘ï¼Œæœºæ¢°ç£¨æŸ |
| é™ä½ç¢°æ’ | RL ä¸çŸ¥ç‰©ç†çº¦æŸ | è¿åŠ¨å—é˜»ï¼Œæ— æ³•ç»§ç»­ |
| æ–¹å‘éš¾åˆ†è¾¨ | æ­¥é•¿ä»… ~3Â° | éš¾ä»¥è§‚å¯Ÿè¿åŠ¨è¶‹åŠ¿ |

### æ–°å¢æ–‡ä»¶

| æ–‡ä»¶ | åŠŸèƒ½ |
|------|------|
| `serial_arm_env_v2.py` | ä¼˜åŒ–ç‰ˆä¸²å£ç¯å¢ƒï¼ˆå¹³æ»‘æ§åˆ¶+é™ä½ä¿æŠ¤ï¼‰|
| `scripts/test_smooth_control.py` | å¹³æ»‘å‚æ•°æµ‹è¯•è„šæœ¬ |

### ä¼˜åŒ–å†…å®¹

#### 1. å¹³æ»‘è¿åŠ¨å‚æ•°

| å‚æ•° | V1 (åŸç‰ˆ) | V2 (ä¼˜åŒ–) | æ•ˆæœ |
|------|-----------|-----------|------|
| `move_time_ms` | 250 | 500 | è¿åŠ¨æ›´æ…¢æ›´å¹³æ»‘ |
| `action_delay_ms` | ~300 | 600 | ç­‰å¾…è¿åŠ¨å®Œæˆå†ä¸‹ä¸€æ­¥ |
| `joint_step_rad` | 0.05 (~3Â°) | 0.12 (~7Â°) | æ–¹å‘æ›´æ˜æ˜¾ |

#### 2. è½¯é™ä½ä¿æŠ¤

```python
# è½¯é™ä½è®¡ç®—
soft_min = hard_min + range * soft_limit_margin  # é»˜è®¤ 10%
soft_max = hard_max - range * soft_limit_margin

# åŠ¨ä½œé¢„åˆ¤
if target_ticks < soft_min or target_ticks > soft_max:
    target_ticks = clamp(target_ticks, soft_min, soft_max)
    # ä¸æ‹’ç»åŠ¨ä½œï¼Œä½†é™åˆ¶åˆ°å®‰å…¨èŒƒå›´
```

#### 3. ä¸‰ç§å¹³æ»‘åº¦é¢„è®¾

```python
# ä½¿ç”¨æ–¹æ³•
cfg = create_smooth_config(port, smoothness="medium")
```

| é¢„è®¾ | `step_rad` | `move_time_ms` | `delay_ms` | é€‚ç”¨åœºæ™¯ |
|------|------------|----------------|------------|----------|
| `low` | 0.10 | 300 | 400 | å¿«é€Ÿæµ‹è¯• |
| `medium` | 0.12 | 500 | 600 | å¹³è¡¡ï¼ˆé»˜è®¤ï¼‰|
| `high` | 0.15 | 800 | 1000 | æ¼”ç¤º/å½•åˆ¶ |

### ä½¿ç”¨æ–¹æ³•

```bash
# æµ‹è¯•å¹³æ»‘å‚æ•°
sudo chmod 666 /dev/ttyACM1

# ä½¿ç”¨é¢„è®¾
python scripts/test_smooth_control.py --port /dev/ttyACM1 --smoothness high

# æ‰‹åŠ¨æŒ‡å®šå‚æ•°
python scripts/test_smooth_control.py --port /dev/ttyACM1 \
    --move-time 600 --action-delay 800 --step-rad 0.15

# æ‰‹åŠ¨æ§åˆ¶æ¨¡å¼
python scripts/test_smooth_control.py --port /dev/ttyACM1 --pattern manual

# RL æ§åˆ¶ä½¿ç”¨å¹³æ»‘ç¯å¢ƒ
python scripts/rl_physical_control.py \
    --serial-port /dev/ttyACM1 \
    --subject 1 --dataset A \
    --smoothness high \
    --use-smooth-env \
    --pre-home --post-home
```

### æµ‹è¯•å›¾æ¡ˆ

| å›¾æ¡ˆ | åŠ¨ä½œåºåˆ— | ç”¨é€” |
|------|----------|------|
| `square` | L L U U R R D D | æµ‹è¯• 4 æ–¹å‘å¾ªç¯ |
| `cross` | L R U D | å¿«é€Ÿæ–¹å‘åˆ‡æ¢ |
| `random` | éšæœº | å‹åŠ›æµ‹è¯• |
| `manual` | äº¤äº’å¼ | æ‰‹åŠ¨æ¢ç´¢é™ä½ |

### è¾“å‡ºæ–‡ä»¶

| æ–‡ä»¶ | æè¿° |
|------|------|
| `serial_arm_env_v2.py` | ä¼˜åŒ–ç‰ˆç¯å¢ƒç±» |
| `scripts/test_smooth_control.py` | æµ‹è¯•è„šæœ¬ |

---

## æœªæ¥è®¡åˆ’

### å¾…å®ç° (æ•™æˆä»»åŠ¡)

- [x] **Task 2**: Controller/Limiter - å…³èŠ‚é™ä½ä¿æŠ¤ âœ… V2.3
- [x] **Task 3**: Smoother + Delay - å‡å°‘é«˜é¢‘é¢¤åŠ¨ âœ… V2.3
- [x] ~~**Task 4**: åŒæ•°æ®é›†å¯¹æ¯” (IV-2b + GigaScience)~~ â†’ å·²æ”¹ä¸º PhysioNet
- [ ] **Task 6**: æ‰©å±•åŠ¨ä½œç©ºé—´ (å¼ å¼€/é—­åˆ, ç§»åŠ¨)

### å¯èƒ½çš„æ”¹è¿›æ–¹å‘

1. **Prioritized Experience Replay** - ä¼˜å…ˆé‡‡æ ·é‡è¦ç»éªŒ
2. **Dueling DQN** - åˆ†ç¦»çŠ¶æ€ä»·å€¼å’Œä¼˜åŠ¿å‡½æ•°
3. **Noisy Networks** - å‚æ•°åŒ–æ¢ç´¢
4. **Multi-step Learning** - n-step returns
5. **Distributional RL** - C51 / QR-DQN

---

## å‚è€ƒæ–‡çŒ®

1. Mnih et al., "Human-level control through deep reinforcement learning", Nature 2015
2. Van Hasselt et al., "Deep Reinforcement Learning with Double Q-learning", AAAI 2016
3. Chen et al., "Decision Transformer: Reinforcement Learning via Sequence Modeling", NeurIPS 2021
4. Vaswani et al., "Attention Is All You Need", NeurIPS 2017

---

*æœ¬æ–‡æ¡£ç”± AI åŠ©æ‰‹è‡ªåŠ¨ç”Ÿæˆï¼Œå¹¶å°†éšä»£ç æ›´æ–°æŒç»­ç»´æŠ¤ã€‚*

